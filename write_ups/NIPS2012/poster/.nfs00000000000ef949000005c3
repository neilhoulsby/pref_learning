%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX poster template
% Created by Nathaniel Johnston
% August 2009
% http://www.nathanieljohnston.com/2009/08/latex-poster-template/
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[final]{beamer}
\usepackage[scale=0.43,orientation=landscape,size=a2]{beamerposter}
\usepackage{graphicx} % allows us to import images

%-----------------------------------------------------------
% Define the column width and poster size
% To set effective sepwid, onecolwid and twocolwid values, first choose how many columns you want and how much separation you want between columns
% The separation I chose is 0.024 and I want 4 columns
% Then set onecolwid to be (1-(3+1)*0.02)/3 = 0.307
% Set twocolwid to be 2*onecolwid + sepwid = 0.613
%-----------------------------------------------------------

\newlength{\sepwid}
\newlength{\onecolwid}
\newlength{\twocolwid}
\newlength{\threecolwid}
\setlength{\sepwid}{0.02\paperwidth}
\setlength{\onecolwid}{0.307\paperwidth}
\setlength{\topmargin}{-0.7in}
\usetheme{confposter}
\usepackage{exscale}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{pgfplots}
\usepackage{array}
%\usepackage{natbib}

\newcommand{\argmax}{ \operatorname*{arg \max}}
\newcommand{\argmin}{ \operatorname*{arg \min}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\pair}{(\x,\x')}
\newcommand{\param}{\bm{\theta}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{y}
\newcommand{\data}{\mathcal{D}}
\newcommand{\h}{\mathbf{H}}
\newcommand{\g}{\mathbf{G}}
\newcommand{\w}{\mathbf{W}}
\newcommand{\pr}{\mathrm{P}}
\newcommand{\ent}{\mathrm{H}}
\newcommand{\info}{\mathrm{I}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\T}{\mathrm{T}}
\newcommand{\ie}{i.\,e.\ }
\newcommand{\eg}{e.\,g.\ }
\newcommand{\latfun}{f}
 \newcommand{\List}{\mathcal{L}}


\definecolor{mycolor1}{rgb}{0.8,0.8,0}
\definecolor{mycolor2}{rgb}{0,1,1}
\definecolor{mycolor3}{rgb}{1,0,1}
\definecolor{mycolor4}{rgb}{1,0.8,0.5}
\definecolor{mycolor5}{rgb}{0.7,0.4,0.01}

%-----------------------------------------------------------
% The next part fixes a problem with figure numbering. Thanks Nishan!
% When including a figure in your poster, be sure that the commands are typed in the following order:
% \begin{figure}
% \includegraphics[...]{...}
% \caption{...}
% \end{figure}
% That is, put the \caption after the \includegraphics
%-----------------------------------------------------------

\usecaptiontemplate{
\small
\structure{\insertcaptionname~\insertcaptionnumber:}
\insertcaption}

%-----------------------------------------------------------
% Define colours (see beamerthemeconfposter.sty to change these colour definitions)
%-----------------------------------------------------------

\setbeamercolor{block title}{fg=blue,bg=white}
\setbeamercolor{block body}{fg=black,bg=white}
\setbeamercolor{block alerted title}{fg=white,bg=dblue!70}
\setbeamercolor{block alerted body}{fg=black,bg=dblue!10}

%-----------------------------------------------------------
% Name and authors of poster/paper/research
%-----------------------------------------------------------

\title{Collaborative Gaussian Processes for Preference Learning}
\author{Neil Houlsby, Jose Miguel Hern\'{a}ndez-Lobato, Ferenc Husz\'{a}r, Zoubin Ghahramani}
\institute{Computational and Biological Learning Lab, Department of Engineering, University of Cambridge}

%-----------------------------------------------------------
% Start the poster itself
%-----------------------------------------------------------

\begin{document}

\begin{frame}[t]
  \begin{columns}[t]

  \begin{column}{\sepwid}\end{column}         % empty spacer column

  \begin{column}{\onecolwid}

    \begin{block}{Multi-user Preference Learning}
        \begin{itemize}
          \item Model a user with a latent `utility' function $f(\x_i)>f(\x_j)$ indicated item $i$
            is preferred to item $j$.
          \item Gaussian process models are popular for \emph{single-user} preference learning
            \cite{chu2005}.
          \item Multiple users: leverage shared behavior.
          \item May or may not have \emph{useful} features for the users.
          \item Current approaches require solving at least $U$ ($=$ number of users) Gaussian
            proccess \cite{bonilla2010, birlutiu2009}.
          \item Goal: build a scalable multi-user probabilistic preference learner that
            may incorporate features if available.
        \end{itemize}
      \end{block}

    \vskip0.5cm

    \begin{alertblock}{The Task}
      \begin{itemize}
        \item {\bf Goal}: build a scalable multi-user probabilistic preference learner that
            may incorporate features if available.
        \item {\bf Approach}: combine dimensionality reducting methods of collaborative filtering
          with flexbile Gaussian process preference function modelling.
        \item {\bf Further desiderata}: efficient inference with preference data,
          `active sampling' of item pairs for efficient data collection.
      \end{itemize}
    \end{alertblock}

    \vskip0.5cm

      \begin{block}{Reformulating Preference Learning as Binary Classification}
        Notation:
        \begin{align}
          \mathbf{x}\in\mathcal{X} &= \text{item feature vector}\notag \\
          y\in\{-1,+1\} &= \text{preference label}\notag \\
          f:\mathcal{X}\mapsto\mathbb{R} &= \text{preference function}\notag \\
          \Phi &= \text{Gaussian c.d.f}\,.\notag
        \end{align}
        GP preference learning \cite{chu2005}:
        \begin{align}
          \mathcal{P}(y|\mathbf{x}_i,\mathbf{x}_j,f) &= \Phi[(f[\mathbf{x}_i] -
            f[\mathbf{x}_j])y]\,, \notag
        \end{align}
        define $g:\mathcal{X}^2\mapsto \mathbb{R}$ as $g(\x_i, \x_j) = f(\x_i) - f(\x_j)$, now
        \begin{align}
          \mathcal{P}(y|\mathbf{x}_i,\mathbf{x}_j,g) &= \Phi[g(\mathbf{x}_i,
          \mathbf{x}_j)y]\,.\label{eqn:likelihood}
        \end{align}
        \begin{itemize}
          \item GP on $f +$ linear operation $\rightarrow$ GP on $g$; derive the \emph{preference
          kernel}:
          \begin{align}
            k_\text{pref}((\x_i,\x_j),(\x_k,\x_l)) = k(\x_i,\x_k) + k(\x_j,\x_l) - k(\x_i,\x_l) -
            k(\x_j,\x_k)\,.\notag
          \end{align}
          \item Enforces anti-symmetric in the prior, will simplify inference greatly.
          \item Note: respects \emph{transitivity}.
        \end{itemize}
      \end{block}

    \end{column}

    \begin{column}{\sepwid}\end{column} % empty spacer column

    \begin{column}{\onecolwid}

      \begin{block}{The Model}
        {\bf Collaborative Gaussian Processes}
        \begin{itemize}
          \item Introduce a set of `shared latent functions':
            $\mathbf{H} = \{h_1\ldots h_D\}$, $D \ll U$.
          \item For the $u$-th user, construct their `user latent function':
            \begin{equation}
              g_{u}(\mathbf{x}_j,\mathbf{x}_k)=\sum_{d=1}^{D}w_{u,d}h_{d}
                (\mathbf{x}_j,\mathbf{x}_k)\,.\notag
            \end{equation}
          \item $\mathbf{W} = \{\{w_{u, d}\}_{u=1}^U\}_{d=1}^D$ are the user-specific weights.
          If user features are available
            $\mathbf{U} = \{\mathbf{u}_1\ \ldots \mathbf{u}_U\}$ replace these with functions:
            $w_d(\mathbf{u})$.
        \end{itemize}
        {\bf Bayesian formulation}

        Data:

         \hskip1cm $\mathcal{L}$ is the list of item pairs evaluated by the users ($P$ pairs),

          \hskip1cm $\mathbf{D}$ is the set of indices of particular items in $\mathcal{L}$ evaluated by each user,

          \hskip1cm $\mathbf{X}, \mathcal{U}$ are the set of item and user features respectively,

          \hskip1cm $\mathbf{T}^{(\mathcal{D})}\in \{-1,+1\}^{U\times P}$
            matrix of preference evaluations corresponding to items in $\mathcal{D}$,

          $\hskip1cm \mathbf{G}^{(\mathcal{D})}\in \mathbb{R}^{U\times P}$
            matrix of `user preference functions',
            evaluated at entries corresponding to observations in $\mathcal{D}$,

          $\hskip1cm \mathbf{H}\in \mathbb{R}^{D\times P}$ matrix of `shared preference functions',

          $\hskip1cm \mathbf{W}\in \mathbb{R}^{U\times D}$ matrix of user weights.
        \begin{itemize}
          \item Likelihood as in \eqref{eqn:likelihood}, and `link function':
            \begin{align*}
              \mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{G}^{(\mathcal{D})})
              = \prod_{u=1}^U \prod_{i=1}^{M_u} \Phi[t_{u,z_{u,i}}
              g_{u,z_{u,i}}] \\
              \mathcal{P}(\mathbf{G}^{(\mathcal{D})}|\mathbf{W},\mathbf{H}) =
              \prod_{u=1}^{U}
              \prod_{i=1}^{M_u}\delta[g_{u,z_{u,i}}-\mathbf{w}_u\mathbf{h}_{\cdot,z_{u,i}}]\,.
            \end{align*}
        \item GP priors on `shared latent functions' and weights:
          \begin{align}
            \mathcal{P}(\mathbf{H}|\mathbf{X},\List) &=
            \prod_{j=1}^{D}\mathcal{N}(\mathbf{h}_j|\mathbf{0},\mathbf{K}_\text{items})\,,\notag \\
            \mathcal{P}(\mathbf{W}|\mathbf{U}) &=
            \prod_{d=1}^D
            \mathcal{N}(\mathbf{w}_{\cdot,d}|\mathbf{0},\mathbf{K}_\text{users})\,,\notag
          \end{align}
          \item Posterior over all parameters:
            \begin{equation}
            \mathcal{P}(\mathbf{W},\mathbf{H},\mathbf{G}^{(\mathcal{D})}|\mathbf{T}^{(\mathcal{D})},\mathbf{X},\List)
            =
            \frac{\mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{G}^{(\mathcal{D})})
            \mathcal{P}(\mathbf{G}^{(\mathcal{D})}|\mathbf{W},\mathbf{H})\mathcal{P}(\mathbf{W}|\mathbf{U})\mathcal{P}(\mathbf{H}|\mathbf{X},\List)}
            {\mathcal{P}(\mathbf{T}^{(\mathcal{D}}|\mathbf{X},\List)}\,.\notag
            \end{equation}
        \end{itemize}
      \end{block}

    \end{column}

      \begin{column}{\sepwid}\end{column}   % empty spacer column

      \begin{column}{\onecolwid}

        \begin{block}{Related Models}
        t
        \end{block}

        \begin{block}{Small Scale Experiments}
        Algorithms:
          \begin{table}[h!]
          \centering
          \begin{tabular}{ll}
            {\bf CPU} & Collaborative Preference (with user features) \\
            {\bf CP} & Collaborative Preference (without user features) \\
            {\bf Bi} & \cite{birlutiu2009} \\
            {\bf Bo} & \cite{bonilla2010} \\
            {\bf SU} & Single-user \cite{chu2005}
          \end{tabular}
          \end{table}
          \input{figs/smallDatasetsCombined.tex}
        \end{block}

        \begin{block}{Large Scale Experiments}
          Algorithms: {\bf *-B} - BALD, {\bf *-E} MES, {\bf \star-R} random sampling.
          \input{figs/resultsTableLargeDatasets.tex}
          \begin{figure}[h!]
          \centering
          \resizebox{\textwidth}{!}{
          \begin{tabular}{ccc}
          Synthetic&
          Sushi&
          MovieLens\\
          \includegraphics[scale=0.3]{figs/error_syntheticDataLargeScale.pdf}&
          \includegraphics[scale=0.3]{figs/error_sushiDataLargeScale.pdf}&
          \includegraphics[scale=0.3]{figs/error_movieLensDataLargeScale.pdf}\\
          Election&
          Jura&
          \\
          \includegraphics[scale=0.3]{figs/error_electionDataLargeScale.pdf}&
          \includegraphics[scale=0.3]{figs/error_juraDataLargeScale.pdf}&
          \hskip0.6cm \raisebox{0.08\height}{\includegraphics[scale=0.45]{figs/legend.pdf}}
          \end{tabular}
          }
          \caption{Average test error for CPU, CP and SU, using the strategies BALD (-B), entropy
          (-E) and random (-R) for active learning.
          For clarity, the curves for CPU are included only in the Synthetic and Election datasets.
          The complete plots can be found in Section 10 of the supplementary
          material.\label{fig:learningcurves}}
          \end{figure}
        \end{block}

        \begin{block}{Acknowledgements}
          NH is a recipient of the Google Europe Fellowship in Statistical Machine Learning, and
          this research is supported in part by this Google Fellowship.
          JMH is supported by Infosys Labs, Infosys Limited.
        \end{block}

        \begin{block}{References}
          {\footnotesize
            \bibliographystyle{hippocampus}
            \bibliography{bib/bibliog}
          }
        \end{block}


      \vskip2.5ex
    \end{column}
  \begin{column}{\sepwid}\end{column}			% empty spacer column
 \end{columns}
\end{frame}
\end{document}
