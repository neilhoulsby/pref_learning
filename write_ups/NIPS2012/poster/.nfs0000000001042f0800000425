%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX poster template
% Created by Nathaniel Johnston
% August 2009
% http://www.nathanieljohnston.com/2009/08/latex-poster-template/
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[final]{beamer}
\usepackage[scale=0.43,orientation=landscape,size=a2]{beamerposter}
\usepackage{graphicx} % allows us to import images

%-----------------------------------------------------------
% Define the column width and poster size
% To set effective sepwid, onecolwid and twocolwid values, first choose how many columns you want and how much separation you want between columns
% The separation I chose is 0.024 and I want 4 columns
% Then set onecolwid to be (1-(3+1)*0.02)/3 = 0.307
% Set twocolwid to be 2*onecolwid + sepwid = 0.613
%-----------------------------------------------------------

\newlength{\sepwid}
\newlength{\onecolwid}
\newlength{\twocolwid}
\newlength{\threecolwid}
\setlength{\sepwid}{0.02\paperwidth}
\setlength{\onecolwid}{0.307\paperwidth}
\setlength{\topmargin}{-0.7in}
\usetheme{confposter}
\usepackage{exscale}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{pgfplots}
\usepackage{array}
%\usepackage{natbib}

\newcommand{\argmax}{ \operatorname*{arg \max}}
\newcommand{\argmin}{ \operatorname*{arg \min}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\pair}{(\x,\x')}
\newcommand{\param}{\bm{\theta}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{y}
\newcommand{\data}{\mathcal{D}}
\newcommand{\h}{\mathbf{H}}
\newcommand{\g}{\mathbf{G}}
\newcommand{\w}{\mathbf{W}}
\newcommand{\pr}{\mathrm{P}}
\newcommand{\ent}{\mathrm{H}}
\newcommand{\info}{\mathrm{I}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\T}{\mathrm{T}}
\newcommand{\ie}{i.\,e.\ }
\newcommand{\eg}{e.\,g.\ }
\newcommand{\latfun}{f}
 \newcommand{\List}{\mathcal{L}}


\definecolor{mycolor1}{rgb}{0.8,0.8,0}
\definecolor{mycolor2}{rgb}{0,1,1}
\definecolor{mycolor3}{rgb}{1,0,1}
\definecolor{mycolor4}{rgb}{1,0.8,0.5}
\definecolor{mycolor5}{rgb}{0.7,0.4,0.01}

%-----------------------------------------------------------
% The next part fixes a problem with figure numbering. Thanks Nishan!
% When including a figure in your poster, be sure that the commands are typed in the following order:
% \begin{figure}
% \includegraphics[...]{...}
% \caption{...}
% \end{figure}
% That is, put the \caption after the \includegraphics
%-----------------------------------------------------------

\usecaptiontemplate{
\small
\structure{\insertcaptionname~\insertcaptionnumber:}
\insertcaption}

%-----------------------------------------------------------
% Define colours (see beamerthemeconfposter.sty to change these colour definitions)
%-----------------------------------------------------------

\setbeamercolor{block title}{fg=blue,bg=white}
\setbeamercolor{block body}{fg=black,bg=white}
\setbeamercolor{block alerted title}{fg=white,bg=dblue!70}
\setbeamercolor{block alerted body}{fg=black,bg=dblue!10}

%-----------------------------------------------------------
% Name and authors of poster/paper/research
%-----------------------------------------------------------

\title{Collaborative Gaussian Processes for Preference Learning}
\author{Neil Houlsby, Jose Miguel Hern\'{a}ndez-Lobato, Ferenc Husz\'{a}r, Zoubin Ghahramani}
\institute{Computational and Biological Learning Lab, Department of Engineering, University of Cambridge}

%-----------------------------------------------------------
% Start the poster itself
%-----------------------------------------------------------

\begin{document}

\begin{frame}[t]
  \begin{columns}[t]
    \begin{column}{\sepwid}\end{column}         % empty spacer column
    \begin{column}{\onecolwid}
      \begin{block}{The problem: Mutli-user Preference Learning}
        \begin{itemize}
          \item Model a user with a latent `utility' function $f(\x_i)>f(\x_j)$ indicated item $i$
            is preferred to item $j$.
          \item Gaussian process models are popular for \emph{single-user} preference learning
            \cite{chu2005}.
          \item Multiple users: leverage shared behavior.
          \item May or may not have \emph{useful} features for the users.
          \item Current approaches require solving at least $U$ ($=$ number of users) Gaussian
            proccess.
          \item Goal: build a scalable multi-user probabilistic preference learner that
            may incorporate features if available.
        \end{itemize}
      \end{block}
      %\vskip2exo

      \begin{block}{Bayesian Information Theoretic AL}
        Latent parameters $\param\in\Theta$ govern dependence of $\y\in\mathcal{Y}$ on input $\x\in\mathcal{X}$ (discriminative model). Observe data, $\data$, Bayes rule yields the posterior distribution over parameters $p(\theta|\data)$.

        Select $\x_i$ (myopically) to minimize the posterior entropy:
        Problems:
   \begin{itemize}
   \item Parameter space is often high dimensional, for GPs, it is infinite dimensional.
   \item Posterior updates required for all input/output combinations ($\mathcal{O}(N_{\x}N_{\y})$).
   \end{itemize}
      \end{block}
      \vskip2ex
      \begin{alertblock}{Solution: Rearrange to Dataspace}
        \vskip-0.5cm
       \vskip-0.3cm
       \begin{itemize}
       \item Output space is often low dimensional and $\mathcal{O}(1)$ posterior updates required.
       \item We call this Bayesian Active Learning by Disagreement (BALD).
       \item Aside: equivalent to the Jensen-Shannon divergence.
       \end{itemize}
      \end{alertblock}
      
    \begin{block}{Review of Gaussian Processes}
      GPs provide a prior over functions $f:\,\mathcal{X}\rightarrow\mathbb{R}$:
      \begin{equation*}
      	f \sim \mathrm{GP}(\mu(\x),k(\x,\x'))
      \end{equation*}
      For regression/classification define likelihood functions respectively:
       \begin{equation*}
        	 \y\vert\x,f \sim\mathcal{N}(f(\x), \sigma^2), \quad \y\vert\x,f \sim\mathrm{Bernoulli}(\Phi(f(\x))) \,\, \Phi(z) = \int_{-\infty}^z\mathcal{N}(0,1)\mathrm{d}z 
       \end{equation*}
      For classification, posterior is intractable, make a Gaussian approximation (Expectation Propagation (EP), the Laplace approximation, Variational methods).
        \begin{figure}
        \begin{tabular}{cc}
        %	\includegraphics[scale=0.5]{figs/GPpriordraw.png}&
%	\includegraphics[scale=0.5]{figs/GPpostdraw.png}\\
        \end{tabular}
        \caption{Toy active GPC problem. True generating function is (\ref{GPplot:true}). 15 actively selected samples are drawn using both BALD (\ref{GPplot:BALDsamp}) and Maximum Entropy Sampling (\ref{GPplot:MESsamp}). The predictive distributions from BALD and MES are (\ref{GPplot:BALD}) and (\ref{GPplot:MES}) respectively.}
        \end{figure}
      \end{block}
    \end{column}

    \begin{column}{\sepwid}\end{column}			% empty spacer column
    \begin{column}{\onecolwid}					
      \begin{block}{BALD for GPC}
       Two terms need to be computed:
	where:
	\begin{itemize}
	\item $\stackrel{1}{\approx}$ is a Gaussian approximation to intractable posterior.
	\item $\stackrel{2}{\approx}$ is a squared exponential approximation to $\rmh(\Phi(f_{\x}))$ (binary entropy of Normal cdf).
	\item The objective function is smooth and differentiable.
      \end{itemize}
      \end{block}
      \begin{alertblock}{Summary}
       \begin{enumerate}
         \item Apply an approximate inference algorithm to get $\mu_{\x,\data}$ and $\sigma_{\x,\data}$ for each point of interest $\x$.
         \item Select $\x$ that maximises:
         \begin{equation}
	\mathrm{h} \left( \Phi\left( \frac{\mu_{\x,\data}}{\sqrt{\sigma^2_{\x,\data} + 1}} \right)\right) - \frac{C}{\sqrt{\sigma_{\x,\data}^2 + C^2}}\exp\left(-\frac{\mu_{\x,\data}^2}{2\left(\sigma_{\x,\data}^2 + C^2\right)}\right)
\end{equation}
       \end{enumerate}
      \end{alertblock}
      \begin{block}{Related Algorithms}
      	The following algorithms are closely related, often approximating the BALD objective:
	\begin{itemize}
	\item Uncertainty Sampling \cite{lewis1994} / Maximum Entropy Sampling \cite{sebastiani2000}.
	\item The Informative Vector Machine \cite{lawrence2001}.
	\item Query by Committee \cite{seung1997}.
	\item SVM-based active learning \cite{tong2001}.
	\end{itemize}
      \end{block}
        \begin{block}{Extensions}
         Further work that we have performed:
       \begin{itemize}
         \item Comparison to decision theoretic algorithms.
	\item Multiclass: combine criteria for $K$ one-versus-all classifiers. 
	\item Preference Learning: extend GP methods of \cite{chu2005}.
       \end{itemize}   
       \end{block}
   \end{column}
   
      \begin{column}{\sepwid}\end{column}			% empty spacer column
      \begin{column}{\onecolwid}
       \begin{block}{Results}
       Experiments run on \emph{pool-based} active learning. Test set accuracy plotted is against number of queries.
       \end{block}
            \begin{block}{Acknowledgements}
           This work is made possible by our sponsors: Google, Europe (Neil Houlsby) and Trinity College, Cambridge (Ferenc Husz\'{a}r) and the Wellcome Trust (M\'{a}t\'{e} Lengyel).
          \end{block}       
  
        \begin{block}{References}
          {\footnotesize
            \bibliographystyle{hippocampus}
            \bibliography{bib/bibliog}
          }
        \end{block}


      \vskip2.5ex
    \end{column}
  \begin{column}{\sepwid}\end{column}			% empty spacer column
 \end{columns}
\end{frame}
\end{document}
