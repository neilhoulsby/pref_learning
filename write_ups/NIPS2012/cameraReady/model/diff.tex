\section{Multi-task Preference Learning \label{sec:model}}

In this section we describe a new probabilistic model for multi-task preference learning.
Let us consider a total of $I$ different items with corresponding feature vectors $\mathbf{x}_1,\ldots,\mathbf{x}_I$,
where $\mathbf{x}_i\in \mathcal{X}$ for $i = 1,\ldots,I$. 
Also, let $\mathbf{X}$ be the set containing the feature vectors for the different items, that is, $\mathbf{X} = \{\mathbf{x}_1,\ldots,\mathbf{x}_I\}$.
The items can be combined into a maximum of $I(I-1)/2)$ different pairs. However, we
consider a list of $P\leq I(I-1)/2$ pairs of items $\ell = \{ p_1,\ldots,p_P \}$, where $p_i = (j, k)$ with $1\leq j < k \leq I$ for $i = 1\ldots,P$.
This list contains the pairs of items for which user preferences are available.
Each pair in the list is identified by a number between 1 and $P$. From now on we refer to each pair in the list using its unique number.
Let $\alpha(i)$ denote the first item in pair $i$ and let $\beta(i)$ denote the second item in that pair.
In the multi-task setting of the preference learning problem we are given
$U$ different preference sets $\mathcal{D}=\{\{z_{u,i},y_{u,i}\}_{i=1}^{M_u}\}_{u=1}^{U}\}$,
a list of item pairs $\ell$ and a set of feature vectors $\mathbf{X}$.
Each of the $U$ sets in $\mathcal{D}$ contains the preferences of a different user
where $M_u$ are the number of observations for the $u$-th user, $z_{u,i}$ is
the $i$-th pair evaluated by the $u$-th user, $y_{i,u}=1$ if this user
prefers item $\alpha(z_{u,i})$ to item $\beta(z_{u,i})$ and $y_{i,u}=-1$ otherwise.
The task of interest is to make predictions about the preferences of
the different users on new pairs of items that they have not compared before. 
For solving this problem we assume a generative
model for the target variable $y_{u,i}$ given the pair $z_{u,i}$.
In particular, we assume that there are $U$ latent functions
$g_{1},\ldots,g_{U}$ such that $g_u:\mathcal{X}^{2}\rightarrow\mathcal{\mathbb{R}}$
and $\mathcal{P}(y_{i,u}=1|g_u,z_{u,i})=\Phi[g_{u}(\mathbf{x}_j, \mathbf{x}_k)]$,
where $j=\alpha(z_{u,i})$, $k=\beta(z_{u,i})$ and $\Phi$ is the cumulative distribution function of a standard Gaussian distribution.
The functions $g_{1},\dots,g_{k}$ are assumed to be generated by a linear combination of a reduced number $D$ of
auxiliary latent functions $h_{1},\ldots,h_{D}$ where $D<U$ and $h_d:\mathcal{X}^{2}\rightarrow\mathcal{\mathbb{R}}$
for $d=1,\ldots,D$, namely
\begin{equation}
g_{u}(\mathbf{x}_j,\mathbf{x}_k)=\sum_{d=1}^{D}w_{u,d}h_{d}(\mathbf{x}_j,\mathbf{x}_k)\,,\label{eq:expressionG}
\end{equation}
where $w_{u,d}\in \mathbb{R}$ for $d=1,\ldots,D$ and $u = 1,\ldots,U$.
This means that the preferences of the different users will share
some common structure represented by the latent fuctions $h_{1},\ldots,h_{D}$.

\subsection{Probabilistic Description}

Let $\mathbf{G}$ be an $U\times P$ matrix such that the entry in the $u$-th column and $i$-th row is 
$g_{u,i}= g_u(\mathbf{x}_j,\mathbf{x}_k)$ where $j=\alpha(i)$ and $k=\beta(i)$.
Additionally, let $\mathbf{H}$ be the $D\times P$ matrix such that the entry in the $d$-th row and $i$-th column is 
$h_{d,i}= h_d(\mathbf{x}_j,\mathbf{x}_k)$ where $j=\alpha(i)$ and $k=\beta(i)$.
We also introduce the $U \times D$ matrix $\mathbf{W}$ such that $\mathbf{G} = \mathbf{W} \mathbf{H}$ and
the $U\times P$ target matrix $\mathbf{T}$ given by $\mathbf{T} = \text{sign}[\mathbf{G} + \mathbf{E}]$,
where $\mathbf{E}$ is an $U \times P$ matrix whose entries are sampled i.i.d. from a standard Gaussian distribution and
the function "$\text{sign}$" applied to a matrix produces another matrix in which each entry is the sign of
the corresponding entry in the original matrix. The observations $y_{u,i}$ found in $\mathcal{D}=\{\{z_{u,i},y_{u,i}\}_{i=1}^{M_u}\}_{u=1}^{U}\}$ are
mapped to the corresponding entries of $\mathbf{T}$ using $t_{u,z_{u,i}} = y_{u,i}$ for $u = 1,\ldots,U$ and $i = 1,\ldots,M_u$.
Finally, let $\mathbf{T}^{(\mathcal{D})}$ and $\mathbf{G}^{(\mathcal{D})}$ denote, respectively, the entries of $\mathbf{T}$ and $\mathbf{G}$
corresponding to the observations $y_{u,i}$ found in $\mathcal{D}$.
The likelihood for $\mathbf{G}^{(\mathcal{D})}$ given $\mathbf{T}^{(\mathcal{D})}$ is
\begin{align}
\mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{G}^{(\mathcal{D})}) 
= \prod_{u=1}^U \prod_{i=1}^{M_u} \Phi[t_{u,z_{u,i}} g_{u,z_{u,i}}]\,. \notag
\end{align}
The conditional distribution for $\mathbf{G}^{(\mathcal{D})}$ given $\mathbf{H}$ and $\mathbf{W}$ is
\begin{align}
\mathcal{P}(\mathbf{G}^{(\mathcal{D})}|\mathbf{W},\mathbf{H}) = 
\prod_{u=1}^{U} \prod_{i=1}^{M_u}\delta[g_{u,z_{u,i}}-\mathbf{w}_u\mathbf{h}_{\cdot,z_{u,i}}]\,, \notag
\end{align}
where $\mathbf{w}_u$ is the $u$-th row in $\mathbf{W}$, $\mathbf{h}_{\cdot,i}$ is the $i$-th column in $\mathbf{H}$
and $\delta$ represents a point probability mass at zero.
To complete the probabilistic description of the multi-task preference model
we have to select priors for $\mathbf{W}$ and $\mathbf{H}$. 
The entries in $\mathbf{W}$ are \emph{a priori} sampled independently from standard Gaussian distributions
\begin{equation}
\mathcal{P}(\mathbf{W})=\prod_{u=1}^{U}\prod_{d=1}^D \mathcal{N}(w_{u,d}|0,1)\,.\notag
\end{equation}
Finally, we assume that each auxiliary latent function $h_1,\ldots,h_D$ is sampled \textit{a priori} from a GP
prior with zero mean and covariance function given by a preference kernel. 
Let $\mathbf{K}$ be the $P \times P$ preference covariance 
matrix for the pairs of items contained in $\ell$. The prior for $\mathbf{H}$ is then 
\begin{equation}
\mathcal{P}(\mathbf{H}|\mathbf{X},\ell)=\prod_{j=1}^{D}\mathcal{P}(\mathbf{h}_{j})=
\prod_{j=1}^{D}\mathcal{N}(\mathbf{h}_j|\mathbf{0},\mathbf{K})\,,\notag
\end{equation}
where $\mathbf{h}_j$ is the $j$-th row in $\mathbf{H}$. The resulting posterior distribution for $\mathbf{w}$, $\mathbf{H}$ and $\mathbf{g}$ is
\begin{align}
&\mathcal{P}(\mathbf{W},\mathbf{H},\mathbf{G}^{(\mathcal{D})}|\mathbf{T}^{(\mathcal{D})},\mathbf{X},\ell) = \notag \\
&\frac{\mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{G}^{(\mathcal{D})})
\mathcal{P}(\mathbf{G}^{(\mathcal{D})}|\mathbf{W},\mathbf{H})\mathcal{P}(\mathbf{W})\mathcal{P}
(\mathbf{H}|\mathbf{X},\ell)}{\mathcal{P}(\mathbf{T}^{(\mathcal{D}}|\mathbf{X},\ell)}\,.\label{eq:post}
\end{align}
Given a new item pair $p_{P+1}$, we can compute the predictive distribution for the preference of the $u$-th user ($1 \leq u \leq U$) on this pair as follows:
\begin{align}
&\mathcal{P}(t_{u,P+1}|\mathbf{T}^{(\mathcal{D})},\mathbf{X},\ell,p_{P+1}) = \nonumber\\
&\quad\int \mathcal{P}(t_{u,P+1}|g_{u,P+1}) \mathcal{P}(g_{u,P+1}|\mathbf{w}_u,\mathbf{h}_{\cdot,P+1})\times \nonumber\\
&\qquad\,\,\mathcal{P}(\mathbf{h}_{\cdot,P+1}|\mathbf{H},\mathbf{X},\ell,p_{P+1})\times \nonumber\\
&\qquad\,\,\mathcal{P}(\mathbf{H},\mathbf{W},\mathbf{G}^{(\mathcal{D})}|\mathbf{T}^{(\mathcal{D})},\mathbf{X},\ell)\,d\mathbf{H}\,d\mathbf{W}\,d\mathbf{G}^{(\mathcal{D})}\,,
\label{eq:predictions}
\end{align}
where \DIFdelbegin \DIFdel{$\mathcal{P}(t_{u,P+1}|g_{u,P+1})= \Phi[t_{u,P+1}g_{u,P+1}]$,
$\mathcal{P}(g_{u,P+1}|\mathbf{w}_u,\mathbf{h}_{\cdot,P+1})= \delta[ g_{u,P+1} - \mathbf{w}_u \mathbf{h}_{\cdot,P+1}]$ and 
}\DIFdelend \DIFaddbegin \DIFadd{$\mathcal{P}(t_{u,P+1}|g_{u,P+1})=\Phi[t_{u,P+1}g_{u,P+1}]$,}\\
\DIFadd{$\mathcal{P}(g_{u,P+1}|\mathbf{w}_u,\mathbf{h}_{\cdot,P+1})=\delta[ g_{u,P+1} - \mathbf{w}_u \mathbf{h}_{\cdot,P+1}]$,
}\DIFaddend \begin{align}
&\mathcal{P}(\mathbf{h}_{\cdot,P+1}|\mathbf{H},\mathbf{X},\ell,p_{P+1})= \DIFaddbegin \notag\\
&\quad \DIFaddend \prod_{d=1}^D \mathcal{P}(h_{d,P+1}|\mathbf{h}_d,\mathbf{X},\ell,p_{P+1})\DIFaddbegin \DIFadd{= }\DIFaddend \notag\\ 
&\quad \DIFdelbegin \DIFdel{=}\DIFdelend \prod_{d=1}^D \mathcal{N}(h_{d,P+1}|\mathbf{k}_\star^\text{T}
\mathbf{K}^{-1}\mathbf{h}_d, k_\star - \mathbf{k}_\star^\text{T}  \mathbf{K}^{-1} \mathbf{k}_\star)\,\DIFdelbegin \DIFdel{.
}\DIFdelend \DIFaddbegin \DIFadd{,
}\DIFaddend \label{eq:predictive}
\end{align}
$k_\star$ is the prior variance of $h_d(\mathbf{x}_{\alpha(P+1)}, \mathbf{x}_{\beta(P+1)})$
and $\mathbf{k}_\star$ is a $P$-dimensional vector that contains the prior covariances between $h_d(\mathbf{x}_{\alpha(P+1)}, \mathbf{x}_{\beta(P+1)})$
and $h_d(\mathbf{x}_{\alpha(1)}, \mathbf{x}_{\beta(1)}),\ldots,h_d(\mathbf{x}_{\alpha(P)}, \mathbf{x}_{\beta(P)})$ for $d=1,\ldots,D$.
However, in practice, the computation of (\ref{eq:post}) or (\ref{eq:predictive})
is infeasible and approximate inference methods have to be used. 
For this \DIFdelbegin \DIFdel{, }\DIFdelend \DIFaddbegin \DIFadd{task }\DIFaddend we propose to use \DIFdelbegin \DIFdel{the EPalgorithm }%DIFDELCMD < \citep{Minka2002}%%%
\DIFdelend \DIFaddbegin \DIFadd{expectation propagation (EP) }\citep{Minka2001}\DIFaddend .
Empirical studies \DIFdelbegin \DIFdel{indicate that EP outperforms other alternative methods such as variational Bayes or Markov chain Monte Carlo
in the }\DIFdelend \DIFaddbegin \DIFadd{show that EP obtains state-of-the-art performance 
in the closely }\DIFaddend related problem of GP binary classification \citep{nickisch2008}\DIFaddbegin \DIFadd{.
}

\DIFadd{Ideally, we would like to use the proposed multi-task model for learning user preferences
using as less data as possible. The objective is to use previously seen data for actively querying the different
users about their preferences on the most informative pairs of items }\citep{brochu2007active}\DIFadd{.
In the next section we describe a novel method for the implementation of this active learning strategy.
The proposed method is general and can be applied to standard Gaussian process binary classification problems}\DIFaddend .
