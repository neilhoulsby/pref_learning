\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{bm}
\usepackage{natbib}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{fixltx2e}
\usepackage{dblfloatfix}
\usepackage{gensymb}

\usepackage{graphicx}
\usepackage{subfigure} 
\usepackage{hyperref}

\newcommand{\argmax}{ \operatorname*{arg \max}}
\newcommand{\argmin}{ \operatorname*{arg \min}} 
\newcommand{\x}{\mathbf{x}} 
\newcommand{\pair}{(\x,\x')} 
\newcommand{\param}{\bm{\theta}}
\newcommand{\X}{\mathbf{X}} 
\newcommand{\y}{y}
\newcommand{\data}{\mathcal{D}} 
\newcommand{\h}{\mathbf{H}} 
\newcommand{\g}{\mathbf{G}} 
\newcommand{\w}{\mathbf{W}} 
\newcommand{\pr}{\mathrm{P}} 
\newcommand{\ent}{\mathrm{H}} 
\newcommand{\info}{\mathrm{I}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\T}{\mathrm{T}}
\newcommand{\ie}{i.\,e.\ }
\newcommand{\eg}{e.\,g.\ }
\newcommand{\latfun}{f}

\begin{document}

\title{Collaborative Gaussian Processes for Preference Learning -- Supplementary Material}

\date{}

\author{Jose Miguel Hern\'{a}ndez-Lobato \\ University of Cambridge \\ \texttt{jmh233@cam.ac.uk} \and Neil Houlsby \\ University of Cambridge  \\ \texttt{nmth2@cam.ac.uk} \and Ferenc Husz\'{a}r \\ University of Cambridge \\ \texttt{fh277@cam.ac.uk} \and Zoubin Ghahramani \\ University of Cambridge \\ \texttt{zoubin@eng.cam.ac.uk}}

\maketitle

\section{Update Operations for Expectation Propagation and Variational Bayes}

The first factor to be refined is $\hat{f}_4$. The update operations that minimize
$\text{KL}(\mathcal{Q}^{\setminus 4} f_4 \| \mathcal{Q}^{\setminus 4} \hat{f}_4)$ are given by

\begin{align}
[\hat{v}_{d,i}^{h,4}]_\text{new} & =
\left\{ [v_{d,i}^{h}]_\text{new}^{-1} - [\hat{v}_{d,i}^{h,2} ]_\text{old}^{-1}\right\}^{-1}\,,\\
[\hat{m}_{d,i}^{h,4}]_\text{new} & =
[\hat{v}_{d,i}^{h,4}]_\text{new} \left\{[ m_{d,i}^{h}]_\text{new} 
[v_{d,i}^{h}]_\text{new}^{-1} - [\hat{m}_{d,i}^{h,4}]_\text{old} [\hat{v}_{d,i}^{h,4}]_\text{old}^{-1}\right\}^{-1}\,,
\end{align}

for $d = 1,\ldots,D$ and $i = 1,\ldots,P$,
where the subscripts \emph{new} and \emph{old} denote the parameter value after and before the update, respectively, and
the parameters $[v_{d,i}^{h}]_\text{new}$ and $[ m_{d,i}^{h}]_\text{new}$ are the $i$-th entries in the vectors
$[\mathbf{v}_{d}^{h}]_\text{new}$ and $[\mathbf{m}_{d}^{h}]_\text{new}$ given by

\begin{align}
[\mathbf{v}_{d}^{h}]_\text{new} & = \text{diag}\left[ \bm \Sigma_d^h \right]\,,\label{eq:Sigmah} \\
[\mathbf{m}_{d}^{h}]_\text{new} & = \bm \Sigma_d^h \text{diag}[\hat{\mathbf{v}}_d^{h,2}]^{-1} \hat{\mathbf{m}}_d^{h,2}\label{eq:Sigma2h}\,,
\end{align}

where $[\bm \Sigma_d^h]^{-1} = \mathbf{K}^{-1}_\text{items} + \text{diag}[\hat{\mathbf{v}}_d^{h,2}]^{-1}$
and the vectors
$\hat{\mathbf{m}}_d^{h,2}$ and $\hat{\mathbf{v}}_d^{h,2}$ are $P$-dimensional vectors given by
$\hat{\mathbf{m}}_d^{h,2}=(\hat{m}_{1,d}^{h,2},\ldots,\hat{m}_{P,d}^{h,2})^\text{T}$ and
$\hat{\mathbf{v}}_d^{h,2}=(\hat{v}_{1,d}^{h,2},\ldots,\hat{v}_{P,d}^{h,2})^\text{T}$.

The second factor to be refined by EP is $\hat{f}_3$. The update operations that minimize
$\text{KL}(\mathcal{Q}^{\setminus 3} f_3 \| \mathcal{Q}^{\setminus 3} \hat{f}_3)$ are

\begin{align}
[\hat{v}_{u,d}^{w,3}]_\text{new} & =
\left\{ [v_{u,d}^{w}]_\text{new}^{-1} - [\hat{v}_{u,d}^{w,2} ]_\text{old}^{-1}\right\}^{-1}\,,\\
[\hat{m}_{u,d}^{w,3}]_\text{new} & =
[\hat{v}_{u,d}^{w,3}]_\text{new} \left\{[ m_{u,d}^{w}]_\text{new} 
[v_{u,d}^{w}]_\text{new}^{-1} - [\hat{m}_{u,d}^{w,3}]_\text{old} [\hat{v}_{u,d}^{w,3}]_\text{old}^{-1}\right\}^{-1}\,,
\end{align}

for $u = 1,\ldots,U$ and $d = 1,\ldots,D$,
where the parameters $[v_{u,d}^{w}]_\text{new}$ and $[ m_{u,d}^{w}]_\text{new}$ are the $u$-th entries in the vectors
$[\mathbf{v}_{d}^{w}]_\text{new}$ and $[\mathbf{m}_{d}^{w}]_\text{new}$ given by

\begin{align}
[\mathbf{v}_{d}^{w}]_\text{new} & = \text{diag}\left[ \bm \Sigma_d^w \right]\,,\label{eq:Sigmaw} \\
[\mathbf{m}_{d}^{w}]_\text{new} & = \bm \Sigma_d^w \text{diag}[\hat{\mathbf{v}}_d^{w,2}]^{-1} \hat{\mathbf{m}}_d^{w,2}\label{eq:Sigma2w}\,,
\end{align}

where $[ \bm \Sigma_d^w]^{-1} = \mathbf{K}^{-1}_\text{items} + \text{diag}[\hat{\mathbf{v}}_d^{w,2}]^{-1}$ and
the vectors $\hat{\mathbf{m}}_d^{w,2}$ and $\hat{\mathbf{v}}_d^{w,2}$ are given by
$\hat{\mathbf{m}}_d^{w,2}=(\hat{m}_{1,d}^{w,2},\ldots,\hat{m}_{U,d}^{w,2})^\text{T}$ and
$\hat{\mathbf{v}}_d^{w,2}=(\hat{v}_{1,d}^{w,2},\ldots,\hat{v}_{U,d}^{w,2})^\text{T}$.

The third factor to be refined by EP is $\hat{f}_2$. For this, we follow the approach used by \cite{stern2009}
and first marginalize $f_2\mathcal{Q}^{\setminus 2}$ with respect to $\mathbf{G}^{(\mathcal{D})}$.
The result of this operation is the auxiliary un-normalized distribution $\mathcal{S}(\mathbf{W},\mathbf{H})$ given by

\begin{eqnarray}
\mathcal{S}(\mathbf{W},\mathbf{H}) & = & \int 
\prod_{u=1}^{U} \prod_{i=1}^{M_u}\delta[g_{u,z_{u,i}}-\mathbf{w}_u\mathbf{h}_{\cdot,z_{u,i}}]
\mathcal{Q}^{\setminus 2 }(\mathbf{G}^{(\mathcal{D})},\mathbf{W},\mathbf{H})\,d\mathbf{G}^{(\mathcal{D})}\nonumber\\
& = & \left[\prod_{u=1}^{U}
\prod_{i=1}^{M_u}\mathcal{N}(\mathbf{w}_u\mathbf{h}_{\cdot,z_{u,i}}|\hat{m}^{g,1}_{u,i},\hat{v}^{g,1}_{u,i})\right]
\left[ \prod_{u=1}^U\prod_{d=1}^D \mathcal{N}(w_{u,d}|\hat{m}^{w,3}_{u,d},\hat{v}^{w,3}_{u,d}) \right]\nonumber \\
& & \left[ \prod_{d=1}^D\prod_{i=1}^P \mathcal{N}(h_{d,i}|\hat{m}^{h,4}_{d,i},\hat{v}^{h,4}_{d,i}) \right]\,.
\end{eqnarray}

Let $\mathcal{Q}_{\mathbf{W},\mathbf{H}}$ be the posterior approximation ($\mathcal{Q}(\mathbf{G}^{(\mathcal{D})},\mathbf{W},\mathbf{H})$) after marginalizing out $\mathbf{G}^{(\mathcal{D})}$.
The parameters of $\mathcal{Q}_{\mathbf{W},\mathbf{H}}$, that is, $m_{d,i}^{h}$,
$v_{d,i}^{h}$, $m_{u,d}^{w}$ and $v_{u,d}^{w}$, for $d = 1,\ldots,D$, $u=1,\ldots,U$ and $i = 1,\ldots,P$,
are then optimized to minimize $\text{KL}(\mathcal{Q}_{\mathbf{W},\mathbf{H}}\|\mathcal{S})$.
This can be done very efficiently using the gradient descent method described by \cite{raiko2007}. The resulting EP updates for $\hat{f}_2$ are
given by

\begin{align}
[\hat{v}_{d,i}^{h,2}]_\text{new} & =
\left\{ [v_{d,i}^{h}]_\text{new}^{-1} - [\hat{v}_{d,i}^{h,2} ]_\text{old}^{-1}\right\}^{-1}\,,\\
[\hat{m}_{d,i}^{h,2}]_\text{new} & =
[\hat{v}_{d,i}^{h,2}]_\text{new} \left\{[ m_{d,i}^{h}]_\text{new} 
[v_{d,i}^{h}]_\text{new}^{-1} - [\hat{m}_{d,i}^{h,2}]_\text{old} [\hat{v}_{d,i}^{h,2}]_\text{old}^{-1}\right\}^{-1}\,,\\
[\hat{v}_{u,d}^{w,2}]_\text{new} & =
\left\{ [v_{u,d}^{w}]_\text{new}^{-1} - [\hat{v}_{u,l}^{w,2} ]_\text{old}^{-1}\right\}^{-1}\,,\\
[\hat{m}_{u,d}^{w,2}]_\text{new} & =
[\hat{v}_{u,d}^{w,2}]_\text{new} \left\{[ m_{u,d}^{w}]_\text{new} 
[v_{u,d}^{w}]_\text{new}^{-1} - [\hat{m}_{u,d}^{w,2}]_\text{old} [\hat{v}_{u,d}^{w,2}]_\text{old}^{-1}\right\}^{-1}\,,\\
[\hat{v}_{u,j}^{g,2}]_\text{new} & =
\left\{ [v_{u,j}^{g}]_\text{new}^{-1} - [\hat{v}_{u,j}^{g,2} ]_\text{old}^{-1}\right\}^{-1}\,,\\
[\hat{m}_{u,j}^{g,2}]_\text{new} & =
[\hat{v}_{u,j}^{g,2}]_\text{new} \left\{[ m_{u,j}^{g}]_\text{new} 
[v_{u,j}^{g}]_\text{new}^{-1} - [\hat{m}_{u,j}^{g,2}]_\text{old} [\hat{v}_{u,j}^{g,2}]_\text{old}^{-1}\right\}^{-1}\,,
\end{align}

for $d = 1,\ldots,D$, $u = 1,\ldots,U$, $j = 1,\ldots,M_u$ and $i = 1,\ldots,P$ where $[m_{d,i}^{h}]_\text{new}$,
$[v_{d,i}^{h}]_\text{new}$, $[ m_{u,d}^{w}]_\text{new}$ and $[v_{u,d}^{w}]_\text{new}$, are the parameters of $\mathcal{Q}$ that minimize
$\text{KL}(\mathcal{Q}_{\mathbf{W},\mathbf{H}}\|\mathcal{S})$ and

\begin{align}
[m_{u,j}^{g}]_\text{new} & = \sum_{d=1}^D [m_{u,d}^{w}]_\text{new} [m_{d,z_{u,j}}^{h}]_\text{new}\,,\\
[v_{u,j}^{g}]_\text{new} & = \sum_{d=1}^D [m_{u,d}^{w}]_\text{new}^2 [v_{d,z_{u,j}}^{h}]_\text{new} +
\sum_{d=1}^D [v_{u,d}^{w}]_\text{new} [m_{d,z_{u,j}}^{h}]_\text{new}^2 +
\sum_{d=1}^D [v_{u,d}^{w}]_\text{new} [v_{d,z_{u,j}}^{h}]_\text{new}\,.
\end{align}

The last factor to be refined on each cycle of EP is $\hat{f}_1$. The EP update operations for this factor are

\begin{align}
[\hat{m}_{u,i}^{g,1}]_\text{new} & = \hat{m}_{u,i}^{g,2} + \hat{v}_{u,i}^{g,2}
[m_{u,i}]^{-1}_\text{new}\,,\\
[\hat{v}_{u,i}^{g,1}]_\text{new} & = \hat{v}_{u,i}^{g,2} \left[ \alpha_{u,i}^{-1} [m_{u,i}]^{-1}_\text{new} - 1 \right]\,,
\end{align}

for $u = 1,\ldots,U$ and $i = 1,\ldots,M_u$, where

\begin{align}
[m_{u,i}]_\text{new} & = \hat{m}_{u,i}^{g,2} + \hat{v}_{u,i}^{g,2} \alpha_{u,i}\,,\\
\alpha_{u,i} & = \Phi[\beta_{u,i}]^{-1} \phi[\beta_{u,i}] t_{u,i} [\hat{v}_{u,i}^{g,2} + 1]^{-\frac{1}{2}}\,,\\
\beta_{u,i} & = t_{u,i} \hat{m}_{u,i}^{g,2} [\hat{v}_{u,i}^{g,2} + 1]^{-\frac{1}{2}}
\end{align}

and $\phi$ and $\Phi$ are the density and the cumulative probability functions of a standard Gaussian distribution,
respectively.

\subsection{The EP Approximation of the Model Evidence}

The model evidence, or marginal likelihood is given by $\mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{X},\ell)$. It is a useful quantity for assessing the quality of the model, and provides a formal tool for selecting hyper-parameters.
Although, like the posterior, it cannot be computed analytically, once EP has converged, we can approximate it using

\begin{align}
\mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{X},\ell) \approx \int \prod_{a=1}^4
\hat{f}_a(\mathbf{G}^{(\mathcal{D})},\mathbf{W},\mathbf{H})\,d\mathbf{G}^{(\mathcal{D})}\,d\mathbf{H}\,d\mathbf{W}\,.
\end{align}

For this, we have to compute the value of the parameters $\hat{s}_1,\ldots,\hat{s}_4$. The value of $\hat{s}_1$ is

\begin{equation}
\log\hat{s}_1 = \sum_{u=1}^{U}\sum_{i=1}^{M_u}\left[\log \Phi[\beta_{u,i}] +
\frac{1}{2}\log(2\pi)+\frac{1}{2}\log \frac{\hat{v}_{u,i}^{g,1} \hat{v}_{u,i}^{g,2}}{v_{u,i}^g}-
\frac{[m_{u,i}^g]^2}{2v_{u,i}^g}+\frac{[\hat{m}_{u,i}^{g,1}]^2}{2\hat{v}_{u,i}^{g,1}}+
\frac{[\hat{m}_{u,i}^{g,2}]^2}{2\hat{v}_{u,i}^{g,2}}\right]\,.
\end{equation}

The value of $\hat{s}_2$ is given by

\begin{align}
\log\hat{s}_2 & = \log Z_2 + \sum_{u=1}^{U}\sum_{i=1}^{M_u}\left[\frac{1}{2}\log(2\pi)+
\frac{1}{2}\log \frac{\hat{v}_{u,i}^{g,1}\hat{v}_{u,i}^{g,2}}{v_{u,i}^g}-
\frac{[m_{u,i}^g]^2}{2v_{u,i}^g}+\frac{[\hat{m}_{u,i}^{g,1}]^2}{2\hat{v}_{u,i}^{g,1}}+
\frac{[\hat{m}_{u,i}^{g,2}]^2}{2\hat{v}_{u,i}^{g,2}}\right]+\nonumber\\
& \quad \sum_{d=1}^{D}\sum_{i=1}^{P}\left[\frac{1}{2}\log(2\pi)+
\frac{1}{2}\log \frac{\hat{v}_{d,i}^{h,2}\hat{v}_{d,i}^{h,4}}{v_{d,i}^h}-
\frac{[m_{d,i}^h]^2}{2v_{d,i}^h}+\frac{[\hat{m}_{d,i}^{h,2}]^2}{2\hat{v}_{d,i}^{h,2}}+
\frac{[\hat{m}_{d,i}^{h,4}]^2}{2\hat{v}_{d,i}^{h,4}}\right]+\nonumber\\
& \quad \sum_{u=1}^{U}\sum_{d=1}^{D}\left[\frac{1}{2}\log(2\pi)+
\frac{1}{2}\log \frac{\hat{v}_{u,d}^{w,2}\hat{v}_{u,d}^{w,3}}{v_{u,d}^w}-
\frac{[m_{u,d}^w]^2}{2v_{u,d}^w}+\frac{[\hat{m}_{u,d}^{w,2}]^2}{2\hat{v}_{u,d}^{w,2}}+
\frac{[\hat{m}_{u,d}^{w,3}]^2}{2\hat{v}_{u,d}^{w,3}}\right]\,,
\end{align}

where $Z_2$ is the variational lower bound obtained in the update of $\hat{f}_2$, that is,

\begin{equation}
Z_2 = \int \mathcal{Q}_{\mathbf{W},\mathbf{H}}\log\frac{\mathcal{S}(\mathbf{W},\mathbf{H})}
{\mathcal{Q}_{\mathbf{W},\mathbf{H}}(\mathbf{W},\mathbf{H})} \,d\mathbf{W},d\mathbf{H}\,.
\end{equation}

The value of $\tilde{s}_{3}$ is given by

\begin{align}
\log\hat{s}_3 = \log Z_3 + \sum_{d=1}^{D}\sum_{u=1}^{U}\left[
\frac{1}{2}\log(2\pi)+\frac{1}{2}\log \frac{\hat{v}_{u,d}^{w,3} \hat{v}_{u,d}^{w,2}}{v_{u,d}^w}-
\frac{[m_{u,d}^w]^2}{2v_{u,d}^w}+\frac{[\hat{m}_{u,d}^{w,3}]^2}{2\hat{v}_{u,d}^{w,3}}+
\frac{[\hat{m}_{u,d}^{w,2}]^2}{2\hat{v}_{u,d}^{w,2}}\right]\,,
\end{align}

where $Z_3$ is computed using

\begin{eqnarray}
\log Z_{3} & = & \log \int \mathcal{P}(\mathbf{W}|\mathbf{U})\left[\prod_{u=1}^{U}\prod_{d=1}^{D}
\mathcal{N}(w_{u,d}|\hat{m}_{u,d}^{w,2},\hat{m}_{u,d}^{w,2})\right]d\mathbf{W}\nonumber\\
& = & -\frac{DP}{2}\log(2\pi) + \frac{1}{2}\sum_{d=1}^D\log|\bm \Sigma_d^w| -
\frac{D}{2}\log|\mathbf{K}_\text{users}| - \frac{1}{2} \sum_{u=1}^U\sum_{d=1}^D \log \hat{v}_{u,d}^{w,2} - \nonumber \\
& & \frac{1}{2} \sum_{u=1}^U\sum_{d=1}^D \frac{[\hat{m}_{u,d}^{w,2}]^2}{\hat{v}_{u,d}^{w,2}} +
\frac{1}{2} \sum_{d=1}^D [\mathbf{m}_d^w]^\text{T} \bm [\Sigma_d^w]^{-1} \mathbf{m}_d^w\,,\label{eq:Z3}
\end{eqnarray}

and $[ \bm \Sigma_d^w]^{-1} = \mathbf{K}^{-1}_\text{users} + \text{diag}[\hat{\mathbf{v}}_d^{w,2}]^{-1}$,
$\mathbf{m}_d^w = \bm \Sigma_d^w \text{diag}[\hat{\mathbf{v}}_d^{w,2}]^{-1} \hat{\mathbf{m}}_d^{w,2}$
and the vectors $\hat{\mathbf{m}}_d^{w,2}$ and $\hat{\mathbf{v}}_d^{w,2}$ are given by
$\hat{\mathbf{m}}_d^{w,2}=(\hat{m}_{1,d}^{w,2},\ldots,\hat{m}_{U,d}^{w,2})^\text{T}$ and
$\hat{\mathbf{v}}_d^{w,2}=(\hat{v}_{1,d}^{w,2},\ldots,\hat{v}_{U,d}^{w,2})^\text{T}$.
Finally, the value of $\tilde{s}_{4}$ is given by
\begin{align}
\log\hat{s}_4 = \log Z_4 + \sum_{d=1}^{D}\sum_{i=1}^{P}\left[
\frac{1}{2}\log(2\pi)+\frac{1}{2}\log \frac{\hat{v}_{d,i}^{h,4} \hat{v}_{d,i}^{h,2}}{v_{d,i}^h}-
\frac{[m_{d,i}^h]^2}{2v_{d,i}^h}+\frac{[\hat{m}_{d,i}^{h,4}]^2}{2\hat{v}_{d,i}^{h,4}}+
\frac{[\hat{m}_{d,i}^{h,2}]^2}{2\hat{v}_{d,i}^{h,2}}\right]\,,
\end{align}

where $Z_4$ is computed using

\begin{eqnarray}
\log Z_{4}  & = & \log \int \mathcal{P}(\mathbf{H}|\mathbf{X},\ell)\left[\prod_{d=1}^{D}\prod_{i=1}^{P}
\mathcal{N}(h_{d,i}|\hat{m}_{d,i}^{h,2},\hat{m}_{d,i}^{h,2})\right]d\mathbf{H}\nonumber\\
& = & -\frac{DP}{2}\log(2\pi) + \frac{1}{2}\sum_{d=1}^D\log|\bm \Sigma_d^h| -
\frac{D}{2}\log|\mathbf{K}_\text{items}| - \frac{1}{2} \sum_{d=1}^D\sum_{i=1}^P \log \hat{v}_{d,i}^{h,2} - \nonumber \\
& & \frac{1}{2} \sum_{d=1}^D\sum_{i=1}^p \frac{[\hat{m}_{d,i}^{h,2}]^2}{\hat{v}_{d,i}^{h,2}} +
\frac{1}{2} \sum_{d=1}^D [\mathbf{m}_d^h]^\text{T} [\bm \Sigma_d^h]^{-1} \mathbf{m}_d^h\,,\label{eq:Z4}
\end{eqnarray}

and $[\bm \Sigma_d^h]^{-1} = \mathbf{K}^{-1}_\text{items} + \text{diag}[\hat{\mathbf{v}}_d^{h,2}]^{-1}$,
$\mathbf{m}_d^h = \bm \Sigma_d \text{diag}[\hat{\mathbf{v}}_d^{h,2}]^{-1} \hat{\mathbf{m}}_d^{h,2}$
and the vectors $\hat{\mathbf{m}}_d^{h,2}$ and $\hat{\mathbf{v}}_d^{h,2}$ are given by
$\hat{\mathbf{m}}_d^{h,2}=(\hat{m}_{1,d}^{h,2},\ldots,\hat{m}_{P,d}^{h,2})^\text{T}$ and
$\hat{\mathbf{v}}_d^{h,2}=(\hat{v}_{1,d}^{h,2},\ldots,\hat{v}_{P,d}^{h,2})^\text{T}$.
Given $\hat{s}_1,\ldots,\hat{s}_4$, we approximate $\mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{X},\ell)$ using

\begin{eqnarray}
\log \mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{X},\ell) & \approx & \sum_{i=a}^{4}\log\hat{s}_{a}-
\sum_{u=1}^{U}\sum_{i=1}^{M_u}\left[\frac{1}{2}\log(2\pi)+\frac{1}{2}\log \frac{\hat{v}_{u,i}^{g,1} \hat{v}_{u,i}^{g,2}}{v_{u,i}^g}-
\frac{[m_{u,i}^g]^2}{2v_{u,i}^g}+\frac{[\hat{m}_{u,i}^{g,1}]^2}{2\hat{v}_{u,i}^{g,1}}+
\frac{[\hat{m}_{u,i}^{g,2}]^2}{2\hat{v}_{u,i}^{g,2}}\right]-\nonumber\\
& & \sum_{d=1}^{D}\sum_{i=1}^{P}\left[
\frac{1}{2}\log(2\pi)+\frac{1}{2}\log \frac{\hat{v}_{d,i}^{h,4} \hat{v}_{d,i}^{h,2}}{v_{d,i}^h}-
\frac{[m_{d,i}^h]^2}{2v_{d,i}^h}+\frac{[\hat{m}_{d,i}^{h,4}]^2}{2\hat{v}_{d,i}^{h,4}}+
\frac{[\hat{m}_{d,i}^{h,2}]^2}{2\hat{v}_{d,i}^{h,2}}\right]-\nonumber\\
& & \sum_{u=1}^{U}\sum_{d=1}^{D}\left[\frac{1}{2}\log(2\pi)+
\frac{1}{2}\log \frac{\hat{v}_{u,d}^{w,2}\hat{v}_{u,d}^{w,3}}{v_{u,d}^w}-
\frac{[m_{u,d}^w]^2}{2v_{u,d}^w}+\frac{[\hat{m}_{u,d}^{w,2}]^2}{2\hat{v}_{u,d}^{w,2}}+
\frac{[\hat{m}_{u,d}^{w,3}]^2}{2\hat{v}_{u,d}^{2,3}}\right]\,.\label{eq:EPevidenceApprox}
\end{eqnarray}

Finally, some of the EP updates may generate a negative value for $\hat{v}_{u,i}^{g,a}$, 
$\hat{v}_{u,d}^{w,a}$ or $\hat{v}_{d,j}^{h,a}$, where $u = 1,\ldots,U$, $i = 1,\ldots,M_u$, $j = 1,\ldots,P$ and $i = 1,\ldots,4$.
Negative variances in Gaussian approximate factors 
are common in many EP implementations \citep{Minka2001,Minka2002}.
When this happens, the marginals of the approximate factor with negative 
variances are not density functions. Instead, they
are correction factors that compensate the errors in the corresponding marginals of other approximate factors.
However, these negative variances can lead to failure of the proposed EP algorithm.
This may happen when we have to compute $\log|\bm \Sigma_d^h|$ in (\ref{eq:Z4}) and some
of the $\hat{v}_{d,i}^{h,2}$ are negative. In this case, $\Sigma_d^h$ may not be
positive definite and $|\bm \Sigma_d^h|$ may be negative. The result is that EP may no longer be able to approximate the model evidence
since $\log |\bm \Sigma_d^h|$ may not be defined in (\ref{eq:Z4}).
The same may occur for $\log|\bm \Sigma_d^w|$ in (\ref{eq:Z3}).
To address this problem, whenever an EP update yields a negative number for any of the
$\hat{v}_{u,i}^{g,a}$, $\hat{v}_{u,d}^{w,a}$ or $\hat{v}_{d,j}^{h,a}$, we do not update this parameter, nor the corresponding
$\hat{m}_{u,i}^{g,a}$, $\hat{m}_{u,d}^{w,a}$ or $\hat{m}_{d,j}^{h,a}$.

\subsection{Details of the Sparse Approximations} \label{sec:sparse}

The computational cost of EP is determined by the operations needed to refine the approximate factors $\hat{f}_3$ and $\hat{f}_4$.
In particular, computing the vectors $[\mathbf{v}_{d}^{h}]_\text{new}$ and $[\mathbf{m}_{d}^{h}]_\text{new}$ in
(\ref{eq:Sigmah}) and (\ref{eq:Sigma2h}), for $d = 1,\ldots,D$, has cost $\mathcal{O}(DP^3)$. Similarly,
the computation of the vectors $[\mathbf{v}_{d}^{w}]_\text{new}$ and $[\mathbf{m}_{d}^{w}]_\text{new}$ in
(\ref{eq:Sigmaw}) and (\ref{eq:Sigma2w}), for $d = 1,\ldots,D$, has cost $\mathcal{O}(DU^3)$.
These costs can be prohibitive when $P$ or $U$ are very large.
Nevertheless, they can be reduced by using sparse approximations to the covariance matrices $\mathbf{K}_\text{users}$
and $\mathbf{K}_\text{items}$. We use the fully independent training conditional or FITC
approximation, also known as the sparse pseudo-input GP (SPGP) \cite{snelson2006}.
With FITC, the $U \times U$ covariance matrix $\mathbf{K}_\text{users}$ is approximated by
$\mathbf{K}_\text{users}' = \mathbf{Q}_\text{users} + \text{diag}(\mathbf{K}_\text{users}-\mathbf{Q}_\text{users})$,
where $\mathbf{Q}_\text{users} = \mathbf{K}_{\text{users},U,U_0} \mathbf{K}_{\text{users},U_0,U_0}^{-1} \mathbf{K}_{\text{users},U,U_0}^\text{T}$.
In this expression, $\mathbf{K}_{\text{users},U_0,U_0}$ is an $U_0 \times U_0$ covariance matrix given by the evaluation
of the covariance function for the users at all possible pairs of $U_0 < U$ locations
or \emph{user pseudo-inputs} $\{\mathbf{u}'_1,\ldots,\mathbf{u}'_{U_0}\}$,
where $\mathbf{u}'_i \in \mathcal{U}$ for $i = 1,\ldots,U_0$, and
$\mathbf{K}_{\text{users},U,U_0}$ is an $U \times U_0$ matrix with the evaluation of
the covariance function for the users at all possible pairs of original user feature vectors and user pseudo-inputs,
that is, $(\mathbf{u}_i, \mathbf{u}_j')$, for $i = 1,\ldots,U$ and $j = 1,\ldots,U_0$.
Similarly, the $P \times P$ covariance matrix $\mathbf{K}_\text{items}$ is also approximated by
$\mathbf{K}_\text{items}' = \mathbf{Q}_\text{items} + \text{diag}(\mathbf{K}_\text{items}-\mathbf{Q}_\text{items})$,
where $\mathbf{Q}_\text{items} = \mathbf{K}_{\text{items},P,P_0} \mathbf{K}_{\text{items},P_0,P_0}^{-1} \mathbf{K}_{\text{items},P,P_0}^\text{T}$,
$\mathbf{K}_{\text{items},P_0,P_0}$ is a $P_0 \times P_0$ covariance matrix given by the evaluation
of the preference kernel at all possible pairs of $P_0 < P$ locations or
\emph{item-pair pseudo-inputs} $\{(\mathbf{x}'_1,\mathbf{x}''_1),\ldots,(\mathbf{x}'_{P_0},\mathbf{x}''_{P_0})\}$,
where $\mathbf{x}'_i,\mathbf{x}''_i \in \mathcal{X}$ for $i = 1,\ldots,P_0$,
and $\mathbf{K}_{\text{items},P,P_0}$ is a $P \times P_0$ matrix with the evaluation of
the preference kernel at all possible combinations of feature vectors for the original item pairs and item-pair pseudo-inputs,
that is, $((\mathbf{x}_{\alpha(i)},\mathbf{x}_{\beta(i)}), (\mathbf{x}'_j,\mathbf{x}''_j))$, for $i = 1,\ldots,P$ and $j = 1,\ldots,P_0$.

We now describe how to refine the third and fourth approximate factors when $\mathbf{K}_\text{users}$ and
$\mathbf{K}_\text{items}$ are replaced by $\mathbf{K}_\text{users}'$ and $\mathbf{K}_\text{items}'$, respectively.
The required operations are can be efficiently implemented using the formulas
described in \citep{Guzman2007} and \citep{Lazaro2010}.
In particular, let $\mathbf{K}_\text{users}' = \mathbf{D} + \mathbf{P}
\mathbf{R}^\text{T} \mathbf{R} \mathbf{P}^\text{T}$,
where $\mathbf{D} = \text{diag}(\mathbf{K}_\text{users}-\mathbf{Q}_\text{users})$,
$\mathbf{P} = \mathbf{K}_{\text{users},U,U_0}$ and
$\mathbf{R}$ is the upper Cholesky factor of $\mathbf{K}_{\text{users},U_0,U_0}^{-1}$, that is,
$\mathbf{K}_{\text{users},U_0,U_0}^{-1} = \mathbf{R}^\text{T} \mathbf{R}$.
This Cholesky factor can be computed using

\begin{equation}
\mathbf{R} = \text{rot180}(\text{chol}(\text{rot180}(\mathbf{K}_{\text{users},U_0,U_0}))^\text{T} \setminus \mathbf{I})\,,
\end{equation}

where $\mathbf{I}$ is the identity matrix, $\text{rot180}(\cdot)$ rotates an $m \times m$ square matrix $180\degree$
so that the element in position $(i,j)$ is moved to position $(m - i + 1, m - j + 1)$,
$\mathbf{A} \setminus \mathbf{a}$ denotes the solution to the linear system $\mathbf{A} \mathbf{x} = \mathbf{a}$ and
$\text{chol}(\cdot)$ returns the upper Cholesky factor of its argument.
The matrix $\bm \Sigma_d^w $, required to compute the vectors $[\mathbf{v}_{d}^{w}]_\text{new}$ and $[\mathbf{m}_{d}^{w}]_\text{new}$
in (\ref{eq:Sigmaw}) and (\ref{eq:Sigma2w}), can the be encoded efficiently using
$\bm \Sigma_d^w = \mathbf{D}^\text{new}_d + \mathbf{P}^\text{new}_d
[\mathbf{R}^\text{new}_d]^\text{T} \mathbf{R}^\text{new}_d [\mathbf{P}^\text{new}_d]^\text{T}$,
where

\begin{eqnarray}
\mathbf{D}^\text{new}_d & = & \left(\mathbf{I} + \mathbf{D}
\text{diag}[\hat{\mathbf{v}}_d^{w,2}]^{-1} \right)^{-1} \mathbf{D}\,,\\
\mathbf{P}^\text{new}_d & = & \left(\mathbf{I} + \mathbf{D}
\text{diag}[\hat{\mathbf{v}}_d^{w,2}]^{-1} \right)^{-1} \mathbf{P}\,,\\
\mathbf{R}^\text{new}_d & = & \text{rot180}(\text{chol}(\text{rot180}(\mathbf{I} + 
\mathbf{R} \mathbf{P}^\text{T} \text{diag}[\hat{\mathbf{v}}_d^{w,2}]^{-1}
(\mathbf{I} + \mathbf{D} \text{diag}[\hat{\mathbf{v}}_d^{w,2}]^{-1})^{-1}
\mathbf{P} \mathbf{R}^\text{T})^\text{T}) \setminus \mathbf{R}
\end{eqnarray}

and $\hat{\mathbf{v}}_d^{w,2}$ is given by
$\hat{\mathbf{v}}_d^{w,2}=(\hat{v}_{1,d}^{w,2},\ldots,\hat{v}_{U,d}^{w,2})^\text{T}$.
The matrix $\bm \Sigma_d^h $, required to compute the vectors $[\mathbf{v}_{d}^{h}]_\text{new}$ and $[\mathbf{m}_{d}^{h}]_\text{new}$
in (\ref{eq:Sigmah}) and (\ref{eq:Sigma2h}), can the be efficiently encoded in a similar manner.
For this, we only have to replace $\hat{\mathbf{v}}_d^{w,2}$
by $\hat{\mathbf{v}}_d^{h,2}=(\hat{v}_{d,1}^{h,2},\ldots,\hat{v}_{d,P}^{h,2})^\text{T}$ and
$\mathbf{K}_{\text{users},U_0,U_0}$ and $\mathbf{K}_{\text{users},U,U_0}$ by
$\mathbf{K}_{\text{items},P_0,P_0}$ and $\mathbf{K}_{\text{items},P,P_0}$, respectively.
These alternative representations of $\bm \Sigma_d^w$ and $\bm \Sigma_d^h$ allow us to
update $\hat{f}_3$ and $\hat{f}_4$ in $\mathcal{O}(DU_0^2U)$ and $\mathcal{O}(DP_0^2P)$ operations, respectively.

We also describe the new update for $\log Z_3$. Instead of using (\ref{eq:Z3}), we now use the following expression

\begin{eqnarray}
\log Z_3 & = & \sum_{d=1}^D \left[ -\frac{U}{2} \log(2 \pi) +
\log|\mathbf{R}^\text{new}_d| - \log|\mathbf{R}| - 
\frac{1}{2} \sum_{u=1}^U \log \left(\hat{v}_{u,d}^{w,2} + d_u \right) + \right. \notag\\
& & \left. \frac{1}{2} \sum_{u=1}^U \hat{m}_{u,d}^{w,2} ([m_{d}^{w}]_\text{new})_u -
\frac{1}{2} \sum_{u=1}^U \frac{[\hat{m}_{u,d}^{w,2}]^2}{\hat{v}_{u,d}^{w,2}}\right]\,,\label{eq:logZ3fitc}
\end{eqnarray}

where $d_u$ is the $u$-th entry in the diagonal of $\mathbf{D}$ and $([m_{d}^{w}]_\text{new})_u$
is the $u$-th entry in the vector $[\mathbf{m}_{d}^{w}]_\text{new}$.
The analogous update for $\log Z_4$ is given by

\begin{eqnarray}
\log Z_4 & = & \sum_{d=1}^D \left[ -\frac{P}{2} \log(2 \pi) +
\log|\mathbf{R}^\text{new}_d| - \log|\mathbf{R}| - 
\frac{1}{2} \sum_{i=1}^P \log \left(\hat{v}_{d,i}^{h,2} + d_i\right) + \right. \notag\\
& & \left. \frac{1}{2} \sum_{i=1}^P \hat{m}_{d,i}^{h,2} ([m_{d}^{h}]_\text{new})_i -
\frac{1}{2} \sum_{i=1}^P \frac{[\hat{m}_{d,i}^{h,2}]^2}{\hat{v}_{d,i}^{h,2}}\right]\,,\label{eq:logZ4fitc}
\end{eqnarray}

where $d_i$ is the $i$-th entry in the diagonal of $\mathbf{D}$ and $([m_{d}^{h}]_\text{new})_i$
is the $i$-th entry in the vector $[\mathbf{m}_{d}^{h}]_\text{new}$.
Note that $d_i$, $\mathbf{R}$ and $\mathbf{R}^\text{new}_d$ in (\ref{eq:logZ4fitc}) refer to the matrices needed
for working with the efficient encoding of
$\mathbf{K}_\text{items}'$. By contrast, $d_u$, $\mathbf{R}$ and $\mathbf{R}^\text{new}_d$ in (\ref{eq:logZ3fitc}) 
refer to the same matrices, but for working with the efficient encoding of $\mathbf{K}_\text{users}'$.

Finally, to compute the predictive distribution, instead of using:

\begin{align}
m_{d,P+1}^h & = \mathbf{k}_\star^\text{T} \left[ \mathbf{K}_\text{items} +
\text{diag}[\hat{\mathbf{v}}_d^{h,2}] \right]^{-1} \hat{\mathbf{m}}_d^{h,2}\,,\label{eq:predictiveMean}\\
v_{d,P+1}^h & = k_\star - \mathbf{k}_\star^\text{T} \left[ \mathbf{K}_\text{items} +
\text{diag}[\hat{\mathbf{v}}_d^{h,2}] \right]^{-1} \mathbf{k}_\star\,,\label{eq:predictiveVariance}
\end{align}

as in the main text, we use

\begin{align}
m_{d,P+1}^h & = \mathbf{k}_\star^\text{T} \bm \gamma^\text{new}_d\,,\\
v_{d,P+1}^h & = d_\star + \parallel \mathbf{R}^\text{new}_d \mathbf{k}_\star \parallel^2\,,
\end{align}

where $\mathbf{k}_\star$ is a $P_0$-dimensional vector that contains the prior covariances
between $h_d(\mathbf{x}_{\alpha(P+1)}, \mathbf{x}_{\beta(P+1)})$
and the value of latent function $h_d$ at the item-pair pseudo-inputs, that is,
$h_d(\mathbf{x}'_{1}, \mathbf{x}''_{1}),\ldots,h_d(\mathbf{x}'_{P_0}, \mathbf{x}''_{P_0})$,
$\bm \gamma^\text{new}_d = [\mathbf{R}^\text{new}_d]^\text{T} \mathbf{R}_d^\text{new}
[\mathbf{P}_d^\text{new}]^\text{T} \text{diag}[\hat{\mathbf{v}}_d^{h,2}]^{-1}\hat{\mathbf{m}}_d^{h,2}$,
$d_\star = k_\star - \mathbf{p}_\star^\text{T} \mathbf{R}^\text{T} \mathbf{R} \mathbf{p}_\star$ and 
finally, $k_\star$ is the prior variance of $h_d(\mathbf{x}_{\alpha(P+1)}, \mathbf{x}_{\beta(P+1)})$.
Note that in all of these formulas, $\mathbf{R}_d^\text{new}$, $\mathbf{R}_d^\text{new}$ and $\mathbf{R}^\text{T}$ 
refer to the matrices needed for working with the efficient encoding of $\mathbf{K}_\text{items}'$.

{
\bibliographystyle{apalike}
\bibliography{../bib/bibliog}
}

\end{document}
