\section{Related Methods \label{sec:relatedWork}}

\subsection{Multi-User Preference Learning}

\paragraph{Model of Birlutiu et al.}

As in the single-task preference learning model, a different GP classifier is
fitted to the data generated by each user. However, the different classifiers are now connected by a
common GP prior for the latent preference functions which is optimized to fit the data \cite{birlutiu2009}.
Let $g_u$ be the $u$-th user's latent preference function and let 
$\bm g_u$ be the $k$-dimensional vector with the evaluation of this function at all the observed pairs of items, that is, $k = P$.
Let $\bar{\bm \mu}$ and $\bar{\bm \Sigma}$ denote the prior mean and prior covariance matrix of $\bm g_u$.
Then $\bar{\bm \mu}$ and $\bar{\bm \Sigma}$ are iteratively refined by an EM algorithm which iterates the following steps:
\begin{description}
\item[E-step] Estimate the sufficient statistics (mean $\bm \mu_u$ and covariance matrix $\bm \Sigma_u$) of the posterior distribution of $\bm g_u$
for user $u=1,\ldots,U$, given the current estimates at step $t$ of the parameters $\bar{\bm \mu}^{(t)}$ and $\bar{\bm \Sigma}^{(t)}$ of the
common GP prior.
\item[M-step] Re-estimate the parameters of the GP prior using
\begin{align}
\bar{\bm \mu}^{(t+1)} & = \frac{1}{U} \sum_{u=1}^U \bm \mu_u\,,\nonumber\\
\bar{\bm \Sigma}^{(t+1)} & = \frac{1}{U} \sum_{u=1}^U \bm (\bar{\bm \mu}^{(t)} - \bm \mu_u)^\text{T} (\bar{\bm \mu}^{(t)} - \bm \mu_u) + 
\frac{1}{U}\sum_{u=1}^U \bm \Sigma_u \,.\nonumber
\end{align}
\end{description}
On the first iteration of the EM algorithm we fix $\bar{\bm \mu}^{(0)} = \bm 0$ and compute $\bar{\bm \Sigma}^{(0)}$ 
by evaluating a preference covariance function at all the possible pairs of items. This preference covariance function
is generated by a squared exponential kernel with unit lengthscale. 
The computational cost of the EM algorithm is rather high
since each iteration requires the inversion of $U$ covariance matrices of dimension $P \times P$,
where $P$ is the total number of observed item pairs, is $\mathcal{O}(UP^3)$.
As described in Section \ref{sec:sparse}, the cost of our equivalent algorithm 
(i.e. where we do not incorporate user features) is $\mathcal{O}(DP^3)$, which, even before
including the FITC approximation is significantly cheaper becuase $D\ll U$.
To reduce the computational burden, we limit
the number of iterations of the EM algorithm to 20. In our experiments, increasing the number of EM iterations 
above 20 did not lead to improvements in the predictive performance of this method.

Birlutiu's approach has an advantage that it learns the full covariance matrix
over the $P$ item pairs for each user, this makes the model flexible,
but causes the poor scalability with the number of users.
We note that the original implementation in \citep{birlutiu2009} does not
use the preference kernel, and used a sampling-based implementation.
We expect that our implementation of this model that uses
the preference kernel
propegation, which is known to perform well with Gaussian processes \citep{nickisch2008},
will augment its performance, but it provides the fairest comparison of 
the underlying model.

\paragraph{Model of Bonilla et al.}

An alternative multi-user model for learning pairwise user preferences is described in \cite{Bonilla2010}.
This approach is based on the assumption that users with similar characteristics or feature vectors should
have similar preferences. In particular, there is a single large latent function $g$ which depends on both
the features of the two items to be compared, $\mathbf{x}_i$ and $\mathbf{x}_j$, and
the specific feature vector for the user who makes the comparison, $\mathbf{u}_u$.
Within the framework of the preference kernel, the likelihood function for Bonilla's model is
\begin{equation}
\mathcal{P}(y|\mathbf{u}_u,\mathbf{x}_i,\mathbf{x}_j,g) = \Phi(yg(\mathbf{x}_i,\mathbf{x}_j,\mathbf{u}_u)
\end{equation}
and the prior for the utility function $g$ is a Gaussian process with zero mean and covariance function 
\begin{equation}
k_\text{Bonilla}((\mathbf{u}_u,\mathbf{x}_i,\mathbf{x}_j), (\mathbf{u}_s,\mathbf{x}_k,\mathbf{x}_l),) =
k_\text{users}(\mathbf{u}_u,\mathbf{u}_s)k_\text{pref}((\mathbf{x}_i,\mathbf{x}_j),(\mathbf{x}_k,\mathbf{x}_l))\,,\label{eq:covBonilla}
\end{equation}
where $k_\text{pref}$ is a preference kernel and $k_\text{users}$ is a covariance function for user features. This latter
function will be large when $\mathbf{u}_u$ and $\mathbf{u}_s$ are similar to each other and small otherwise.
Therefore, the effect of $k_\text{users}$ in (\ref{eq:covBonilla}) is to 
encourage users with similar feature vectors to
agree on their preferences. The preference kernel allows us to do efficient approximate inference in Bonilla's model
using any standard implementation of expectation propagation for the binary classification problem with GPs.
However, the computational cost of Bonilla's method is rather high. 
When the preference kernel is used, the cost of this technique is $\mathcal{O}((\sum_{u=1}^U M_u)^3)$,
where $U$ is the total number of users and $M_u$ is the number of pairs evaluated by the $u$-th user.
By contrast, our approach, when including user featuers also has complexity $\mathcal{O}(DU^3+DP^3)$. 
In the original work \cite{Bonilla2010} they do not use the preference kernel, this reduces
their complexity to $O((\sum_{u=1}^UI_u)^3)$, where $I_u$ denotes the number of unique items compared
by the $u$-th user. However, this makes inference more complex and they are constrained to using
the Laplace approximation, which is shown to perform less well than EP for with Gaussian Process models \citep{nickisch2008}. Furthermore, this cost is still typically much greater than for our model.
In practice, Bonilla's method is infeasible when
we have observations for more than a few hundred users. Additionally, this method 
requires that feature vectors are available for the different users and that users with similar feature vectors 
generate similar preference observations. When these conditions do not hold, Bonilla's method may
lead to poor predictive performance.
 

\subsection{Active Learning}

\paragraph{Maximum Entropy Sampling} (MES) \citep{sebastiani2000} is similar to BALD in the sense that it also works explicitly in data space
(that is, using equation \eqref{eqn:rearrangement}). MES was proposed for regression models with input-independent observation noise.
In this scenario, the noise in the target variable $y$ does not depend on the input $\mathbf{x}$ and
the second term in equation \eqref{eqn:rearrangement} is constant and can be safely ignored.
However, if noise in the target variable is not input-independent, MES will tend to sample regions of the input space
where uncertainty in $g$ is low but uncertainty in the labels (because of observation noise) is high,
as illustrated in Figure \ref{fig:BALD}.

\paragraph{Query by Committee} (QBC) makes a different approximation to \eqref{eqn:rearrangement} \citep{freund1997}.
QBC samples parameters from the posterior (called committee members). These parameters are then used to perform a deterministic vote on
the outcome of each candidate $\x$. The $\x$ with the most balanced vote is selected for the next active inclusion in the training set.
This objective is termed the `principle of maximal disagreement'. QBC is similar to
BALD when the objective used by BALD is approximated by sampling from the posterior, with the exception
that BALD uses a probabilistic measure of disagreement (equation \eqref{eqn:rearrangement}).
Note that the deterministic vote criterion used by QBC does not take into account
the confidence of the learning method on its predictions. Because of this, QBC can exhibit the same pathologies as MES.

\paragraph{The Informative Vector Machine} (IVM) \citep{lawrence2002} is also motivated by information theory.
This method was originally designed for sub-sampling a dataset and not for addressing online active learning problems.
The IVM requires that the target variables $y$ are observed prior to making a query and it is therefore not applicable online active learning tasks.
Nonetheless, BALD can be applied to the dataset sub-sampling problem for which the IVM is designed, it is simply equipped with less information.
The IVM works with equation \eqref{eqn:ent_change} instead of \eqref{eqn:rearrangement}. Entropies for the latent function $g$ are
calculated approximately in the marginal subspace corresponding to the observed data points.
For this, the IVM employs a Gaussian approximation to the posterior distribution at these locations.
The posterior approximation must be updated to evaluate the entropy decrease after the inclusion
of each candidate data point. If there are $n$ candidate inputs under consideration, a total of $\mathcal{O}(n)$ posterior
updates are required. By contrast, BALD only requires $\mathcal{O}(1)$ updates.
In practice, the IVM approach is infeasible in sophisticated models such as the proposed multi-user approach.

Finally, \cite{tong2001} propose an algorithm for active learning with support vector machines.
This method approximates the version space (the set of hyperplanes consistent with the data)
with a simpler object, such as a hypersphere. The algorithm selects the data point whose
dual plane is closest to bisecting this hypersphere. 

%\paragraph{Expected Value of Information} Another flavour of approaches is proposed for multi-user preference learning is to use the EVOI \citep{bonilla2010}. This is a decision theoretic approach, and the objective is not to learn optimally about the parameters but to seek the data that is expected to improve the maximum of users latent function the most (i.e. find their most preferred item). This of course requires the set or distribution over test items to be known; although in general this will not be true it is a reasonable assumption for a recommendation application. This approach is well tailored for online active learning with a single user as it seeks to find the best recommendation as fast as possible, whereas the information theoretic approach is more suited when trying to learn the model offline from existing data from many users. By focussing just on areas of high preference one is less likely to discover the true latent functions underlying user behaviour, our approach directly tries to reduce uncertainty about these alongside the user-specific parameters (i.e. the weights). Perhaps more importantly to calculate the EVOI new predictive distributions need to be calculated for all possible data points under consideration, again requiring $\mathcal{O}(Q_{\pair}Q_{\y})$ posterior updates; we only require one from that may calculate the score for all candidates, this offers a very large computational saving when the number of possible item pairs is large.

\subsection{Binary Classification Investigation}

We describe an experiment used to to compare BALD to these approaches in the context of binary classification with GPs.
The datasets were divided randomly into pool and test sets.
Each algorithm was initialized with two data points, one from each class, drawn randomly from the pool.
The algorithms select points sequentially, and their classification error was assessed on the test set after each query. The procedure was repeated for several random splits of the data to 
assess statistical significance.
Figure \ref{tab:activeTable} provides a summary of the results. 
BALD can be seen to outperform consistently the alternative algorithms across many datasets. The closest competitor is Maximum Entropy Sampling, which we use as a benchmark active learning algorithm for use with the multi-user preference model in the main paper. 

Table \ref{tab:activeTable} depicts an investigation of BALD's power against the aforementioned algorithms and a non-probabilistic SVM-based active learning algorithm of \citep{tong2001}; this algorithm seeks to reduce the size of `Version Space' maximally fast, which is equivalent to the information theoretic approach but for non-probabilistic models.  

We find that BALD is consistently the best performing algorithm, the closest method is MES, whose performance approaches BALD when the data has very low noise.  
We therefore use MES as our benchmark active learning algorithm for our experiments.

\input{figs/activeGPCtable.tex}
