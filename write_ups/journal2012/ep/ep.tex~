\section{Expectation Propagation and Variational Bayes\label{sec:ep}}

This proposed method for approximate inference in our multi-user model is based on the combination of expectation propagation \cite{Minka2002,gerven2010a}
and variational inference \cite{stern2009}. We first describe the general version of the algorithm.
Finally, in Section \ref{sec:sparse}, we describe the version which employs sparse approximations to the covariance matrices $\mathbf{K}_\text{users}$
and $\mathbf{K}_\text{items}$ for speeding up computations.

The proposed EP method approximates the exact posterior distribution by the following parametric distribution:

\begin{eqnarray}
\mathcal{Q}(\mathbf{G}^{(\mathcal{D})},\mathbf{W},\mathbf{H}) & = & \left[\prod_{u=1}^{U}\prod_{d=1}^{D}\mathcal{N}(w_{ud}|m_{u,d}^{w},v_{u,d}^{w})\right]
\left[\prod_{d=1}^{D}\prod_{i=1}^{P} \mathcal{N}(h_{d,i}|m_{d,i}^h,v_{d,i}^{h})\right]\nonumber\\
& & \left[\prod_{u=1}^N \prod_{j=1}^{M_u} \mathcal{N}(g_{u,z_{u,j}}|m_{u,j}^g,v_{u,j}^g)\right]\,,\label{eq:epPostApprox}
\end{eqnarray}

where $m_{u,d}^w$, $v_{u,d}^w$, $m_{d,i}^h$, $v_{d,i}^h$,
$m_{u,j}^g$, and $v_{u,j}^g$ are free parameters to be determined by EP. 
The joint distribution of the model parameters and the data 
$\mathcal{P}(\mathbf{G}^{(\mathcal{D})},\mathbf{W},\mathbf{H},\mathbf{T}^{(\mathcal{D})},\mathbf{X},\ell)$ can be factorized
into four factors $f_1,\ldots,f_4$, namely,

\begin{equation}
\mathcal{P}(\mathbf{G}^{(\mathcal{D})},\mathbf{W},\mathbf{H},\mathbf{T}^{(\mathcal{D})},\mathbf{X},\ell) =
\prod_{k=1}^4 f_a(\mathbf{G}^{(\mathcal{D})},\mathbf{W},\mathbf{H})\,,
\end{equation}

where $f_1(\mathbf{G}^{(\mathcal{D})},\mathbf{W},\mathbf{H}) = \mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{G}^{(\mathcal{D})})$,
$f_2(\mathbf{G}^{(\mathcal{D})},\mathbf{W},\mathbf{H}) = \mathcal{P}(\mathbf{G}^{(\mathcal{D})}|\mathbf{W},\mathbf{H})$,
$f_3(\mathbf{G}^{(\mathcal{D})},\mathbf{W},\mathbf{H}) = \mathcal{P}(\mathbf{W}|\mathbf{U})$ and
$f_4(\mathbf{G}^{(\mathcal{D})},\mathbf{W},\mathbf{H}) = \mathcal{P}(\mathbf{H}|\mathbf{X},\ell)$.
EP approximates each of these exact factors by 
approximate factors $\hat{f}_{1}(\mathbf{W},\mathbf{H},\mathbf{G}^{(\mathcal{D})}),\ldots,\hat{f}_{4}(\mathbf{W},\mathbf{H},\mathbf{G}^{(\mathcal{D})})$
that have the same functional form as (\ref{eq:epPostApprox}), namely,

\begin{eqnarray}
\hat{f}_a(\mathbf{G}^{(\mathcal{D})},\mathbf{W},\mathbf{H}) & = &
\left[\prod_{u=1}^{U}\prod_{d=1}^{D}\mathcal{N}(w_{ud}|\hat{m}_{u,d}^{a,w},\hat{v}_{u,d}^{a,w})\right]
\left[\prod_{d=1}^{D}\prod_{i=1}^{P} \mathcal{N}(h_{d,i}|\hat{m}_{d,i}^{a,h},\hat{v}_{d,i}^{a,h})\right]\nonumber\\
& & \left[\prod_{u=1}^N \prod_{j=1}^{M_u} \mathcal{N}(g_{u,z_{u,j}}|\hat{m}_{u,j}^{a,g},\hat{v}_{u,j}^{a,g})\right] \hat{s}_a\,,
\end{eqnarray}

where $a=1,\ldots,4$ and $\hat{m}_{u,d}^{a,w}$, $\hat{v}_{u,d}^{a,w}$, $\hat{m}_{d,i}^{a,h}$, $\hat{v}_{d,i}^{a,h}$,
$\hat{m}_{u,j}^{a,g}$, $\hat{v}_{u,j}^{a,g}$ and $\hat{s}_a$ are free parameters to be determined by EP. 
The posterior approximation $\mathcal{Q}(\mathbf{w},\mathbf{H},\mathbf{G}^{(\mathcal{D})})$
is obtained as the normalized product of the approximate factors $\hat{f}_{1},\ldots,\hat{f}_{4}$, that is,

\begin{equation}
\mathcal{Q}(\mathbf{W},\mathbf{H},\mathbf{G}^{(\mathcal{D})}) \propto
\hat{f}_{1}(\mathbf{W},\mathbf{H},\mathbf{G}^{(\mathcal{D})})\cdots\hat{f}_{4}(\mathbf{W},\mathbf{H},\mathbf{G}^{(\mathcal{D})})\,.
\end{equation}

The first step of EP is to initialize all the approximate factors $\hat{f}_1,\ldots,\hat{f}_4$ and the posterior approximation $\mathcal{Q}$ to be uniform.
In particular,
$m_{u,d}^w=m_{d,i}^h=m_{u,j}^g=\hat{m}_{u,d}^{w,a}=\hat{m}_{d,i}^{a,h}=\hat{m}_{u,j}^{g,a}=0$ and 
$v_{u,d}^w = v_{d,i}^h = v_{u,j}^g = \hat{v}_{u,d}^{a,w} = \hat{v}_{d,i}^{a,h} = \hat{v}_{u,j}^{a,h} = \infty$ for
$a=1,\ldots,4$, $u=1,\ldots,U$, $d=1,\ldots,D$, $i=1,\ldots,P$ and $j = 1,\ldots,M_u$.
After that, EP refines the parameters of the approximate factors by iteratively minimizing the Kullback-Leibler (KL) divergence
between $\mathcal{Q}^{\setminus a}(\mathbf{W},\mathbf{H},\mathbf{G}^{(\mathcal{D})})f_a(\mathbf{W},\mathbf{H},\mathbf{G}^{(\mathcal{D})})$
and $\mathcal{Q}^{\setminus a}(\mathbf{W},\mathbf{H},\mathbf{G}^{(\mathcal{D})})\hat{f}_a(\mathbf{W},\mathbf{H},\mathbf{G}^{(\mathcal{D})})$, for $a=1,\ldots,4$,
where $\mathcal{Q}^{\setminus a}$ is the ratio between $\mathcal{Q}$ and $\hat{f}_a$. That is, EP iteratively minimizes

\begin{equation} 
\text{D}_{\text{KL}}(Q^{\setminus a}f_a\|Q^{\setminus a}\hat{f}_a) 
= \int \left[Q^{\setminus a}f_a \log \frac{Q^{\setminus a}f_a}{Q^{\setminus a}\hat{f}_a}+
Q^{\setminus a}\hat{f}_a-Q^{\setminus a}f_a\right]\,d\mathbf{W}\,d\mathbf{H}\,d\mathbf{G}^{(\mathcal{D})}\label{eq:KL}
\end{equation}

with respect to $\hat{f}_a$, for $a = 1,\ldots,4$.
The arguments to $Q^{\setminus a}f_a $ and 
$Q^{\setminus a}\hat{f}_a$ have been omitted in the 
right-hand side of (\ref{eq:KL}) to improve the readability of the expression.
However, the minimization of (\ref{eq:KL}) does not perform well when we have to refine the parameters of $\hat{f}_2$.
The reason for this is that the corresponding exact factor $f_2$ (equation (7) in the main document)
is invariant to simultaneous changes in sign, scalings, or rotations of the entries of $\mathbf{W}$ and $\mathbf{H}$.
This non-identifiability in the latent space spanned by $\mathbf{W}$ and $\mathbf{H}$
originates multiple modes in the distribution $Q^{\setminus 2}f_2$.
The minimization of the direct version of the KL divergence results in an approximation that averages across all
of the modes, leading to poor predictive performance.
We solve this problem by following an approach similar to the one described by \cite{stern2009}.
Instead of minimizing $\text{KL}(\mathcal{Q}^{\setminus 2} f_2 \| \mathcal{Q}^{\setminus 2} \hat{f}_2)$,
we refine $\hat{f}_2$ by minimizing the reversed version of the KL divergence, that is,
we minimize $\text{KL}(\mathcal{Q}^{\setminus 2} \hat{f}_2 \| \mathcal{Q}^{\setminus 2} f_2)$ with respect to the parameters of $\hat{f}_2$.
The reversed version of the divergence has mode seeking properties \citep{Bishop2007} and tends to approximate
only a single mode of the target distribution, leading to better predictive accuracy.

The EP algorithm iteratively refines the approximate factors until convergence.
We assume the algorithm has converged when the absolute value of the change in the parameters
$m_{u,i}^g$ of $\mathcal{Q}$, where $u = 1,\ldots,U$ and $i = 1,\ldots,M_u$,
is less than a threshold $\delta = 10^{-2}$ between two consecutive cycles of EP,
where a cycle consists in the sequential update of all the approximate factors.
However, convergence is not guaranteed and
EP may end up oscillating without ever stopping \citep{Minka2001}.
This undesirable behavior can be prevented by \emph{damping} the EP updates \citep{Minka2002}.
Let $\hat{f}_a^\text{new}$ denote the value of the approximate factor that minimizes the Kullback-Leibler
divergence. Damping consists of using

\begin{equation}
\hat{f}_a^\text{damp} = \left[ \hat{f}_a^\text{new} \right]^\epsilon \left[ \hat{f}_a \right]^{(1 - \epsilon)}\,,\label{eq:damping}
\end{equation}

instead of $\hat{f}_a^\text{new}$ for the update of each approximate factor $a = 1,\ldots,4$.
The quantity $\hat{f}_a$ represents in (\ref{eq:damping})
the factor before the update. The parameter
$\epsilon \in [0,1]$ controls the amount of damping.
The original EP update (that is, without damping)
is recovered in the limit $\epsilon = 1$. For $\epsilon = 0$,
the approximate factor $\hat{f}_a$ is not modified.
To improve the converge of EP, we use a damping scheme
with a parameter $\epsilon$ that is initialized to 1
and then progressively annealed as recommended by \cite{HernandezLobato2010}.
After each iteration of EP, the value of
this parameter is multiplied by a constant $k < 1$.
The value selected for $k$ is $k = 0.95$.
In the experiments performed, EP performs on average about 50 iterations.

\subsection{The EP predictive distribution}

EP can also approximate the predictive distribution, given by equation (11) in the main manuscript. 
For this, we replace the exact posterior with the EP approximation $\mathcal{Q}$. In this way, we obtain

\begin{equation}
\mathcal{P}(t_{u,P+1}|\mathbf{T}^{(\mathcal{D})},\mathbf{X},\ell,p_{P+1}) \approx 
\Phi\left[t_{u,P+1} m_{u,P+1}^g(v_{u,P+1}^g + 1)^{-\frac{1}{2}}\right]\,,
\end{equation}

where

\begin{align}
m_{u,P+1}^g & = \sum_{d=1}^D  m_{u,d}^w m_{d,P+1}^h\,,\\
v_{u,P+1}^g & = \sum_{d=1}^D  [m_{u,d}^w]^2 v_{d,P+1}^h + \sum_{d=1}^D v_{u,d}^w [m_{d,P+1}^h]^2 + \sum_{d=1}^D v_{u,d}^w v_{d,P+1}^h
\end{align}

and $m_{d,P+1}^h$ and $v_{d,P+1}^h$ for $d = 1,\ldots,D$ are given by

\begin{align}
m_{d,P+1}^h & = \mathbf{k}_\star^\text{T} \left[ \mathbf{K}_\text{items} +
\text{diag}[\hat{\mathbf{v}}_d^{h,2}] \right]^{-1} \hat{\mathbf{m}}_d^{h,2}\,,\label{eq:predictiveMean}\\
v_{d,P+1}^h & = k_\star - \mathbf{k}_\star^\text{T} \left[ \mathbf{K}_\text{items} +
\text{diag}[\hat{\mathbf{v}}_d^{h,2}] \right]^{-1} \mathbf{k}_\star\,,\label{eq:predictiveVariance}
\end{align}

where $k_\star$ is the prior variance of $h_d(\mathbf{x}_{\alpha(P+1)}, \mathbf{x}_{\beta(P+1)})$,
$\mathbf{k}_\star$ is a $P$-dimensional vector that contains the prior covariances between $h_d(\mathbf{x}_{\alpha(P+1)}, \mathbf{x}_{\beta(P+1)})$
and $h_d(\mathbf{x}_{\alpha(1)}, \mathbf{x}_{\beta(1)}),\ldots,h_d(\mathbf{x}_{\alpha(P)}, \mathbf{x}_{\beta(P)})$ for $d=1,\ldots,D$,
the function $\text{diag}(\cdot)$ applied to a vector returns a diagonal matrix with that vector in
its diagonal and the vectors $\hat{\mathbf{m}}_d^{h,2}$ and $\hat{\mathbf{v}}_d^{h,2}$ are given by
$\hat{\mathbf{m}}_d^{h,2}=(\hat{m}_{1,d}^{h,2},\ldots,\hat{m}_{P,d}^{h,2})^\text{T}$ and
$\hat{\mathbf{v}}_d^{h,2}=(\hat{v}_{1,d}^{h,2},\ldots,\hat{v}_{P,d}^{h,2})^\text{T}$.

\subsection{The EP update operations}

In this section we describe the EP updates for refining the approximate factors $\hat{f}_1,\ldots,\hat{f}_4$.
For the sake of clarity, we only include the update rules with no damping ($\epsilon = 1$).
Incorporating the effect of damping in these operations is straightforward. 
With damping,  the natural parameters of the approximate factors become 
a convex combination of the natural parameters before and after the 
update with no damping

\begin{align}
[\hat{v}_{u,d}^{w,a}]^{-1}_\text{damp} & = \epsilon [\hat{v}_{u,d}^{w,a}]^{-1}_\text{new}
+ (1 - \epsilon) [\hat{v}_{u,d}^{w,a}]^{-1}_\text{old}\,,\\
[\hat{m}_{u,d}^{w,a}]_\text{damp} [\hat{v}_{u,d}^{w,a}]_\text{damp}^{-1} & =
\epsilon [\hat{m}_{u,d}^{w,a}]_\text{new} [\hat{v}_{u,d}^{w,a}]_\text{new}^{-1} + (1 - \epsilon)
[\hat{m}_{u,d}^{w,a}]_\text{old}[\hat{v}_{u,d}^{w,a}]^{-1}_\text{old}\,,\\
[\hat{v}_{d,i}^{h,a}]^{-1}_\text{damp} & = \epsilon [\hat{v}_{d,i}^{h,a}]^{-1}_\text{new}
+ (1 - \epsilon) [\hat{v}_{d,i}^{h,a}]^{-1}_\text{old}\,,\\
[\hat{m}_{d,i}^{h,a}]_\text{damp} [\hat{v}_{d,i}^{h,a}]_\text{damp}^{-1} & =
\epsilon [\hat{m}_{d,i}^{h,a}]_\text{new} [\hat{v}_{d,i}^{h,a}]_\text{new}^{-1} + (1 - \epsilon)
[\hat{m}_{d,i}^{h,a}]_\text{old}[\hat{v}_{d,i}^{h,a}]^{-1}_\text{old}\,,\\
[\hat{v}_{u,j}^{g,a}]^{-1}_\text{damp} & = \epsilon [\hat{v}_{u,j}^{g,a}]^{-1}_\text{new}
+ (1 - \epsilon) [\hat{v}_{u,j}^{g,a}]^{-1}_\text{old}\,,\\
[\hat{m}_{d,j}^{g,a}]_\text{damp} [\hat{v}_{u,j}^{g,a}]_\text{damp}^{-1} & =
\epsilon [\hat{m}_{u,j}^{g,a}]_\text{new} [\hat{v}_{u,j}^{g,a}]_\text{new}^{-1} + (1 - \epsilon)
[\hat{m}_{u,j}^{g,a}]_\text{old}[\hat{v}_{u,j}^{g,a}]^{-1}_\text{old}\,,
\end{align}

where $u = 1,\ldots,U$, $d = 1,\ldots,D$, $i = 1,\ldots,P$ and $j=1,\ldots,M_u$.
The subscript $\emph{new}$ denotes the value of the parameter 
given by the full EP update operation with no damping.
The subscript $\emph{damp}$ denotes the parameter value given by the
damped update rule. The subscript $\emph{old}$ refers to
the value of the parameter before the EP update.
The updates for the parameters $\hat{s}_1,\ldots,\hat{s}_4$ are not damped. These parameters are initialized to 1 and are only
updated once the EP algorithm has converged.

The factors are refined in the order $\hat{f}_4, \hat{f}_3, \hat{f}_2, \hat{f}_1$ by applying the EP and VB updates described above. Full details of the specific update operations have been relegated to the Supplementary Material.


\subsection{The EP Approximation of the Model Evidence}

The model evidence, or marginal likelihood is given by $\mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{X},\ell)$. It is a useful quantity for assessing the quality of the model, and provides a formal tool for selecting hyper-parameters.
Although, like the posterior, it cannot be computed analytically, once EP has converged, we can approximate it using

\begin{align}
\mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{X},\ell) \approx \int \prod_{a=1}^4
\hat{f}_a(\mathbf{G}^{(\mathcal{D})},\mathbf{W},\mathbf{H})\,d\mathbf{G}^{(\mathcal{D})}\,d\mathbf{H}\,d\mathbf{W}\,.
\end{align}

The required computations are presented in the Supplementary Material.

Finally, some of the EP updates may generate a negative value for $\hat{v}_{u,i}^{g,a}$, 
$\hat{v}_{u,d}^{w,a}$ or $\hat{v}_{d,j}^{h,a}$, where $u = 1,\ldots,U$, $i = 1,\ldots,M_u$, $j = 1,\ldots,P$ and $i = 1,\ldots,4$.
Negative variances in Gaussian approximate factors 
are common in many EP implementations \citep{Minka2001,Minka2002}.
When this happens, the marginals of the approximate factor with negative 
variances are not density functions. Instead, they
are correction factors that compensate the errors in the corresponding marginals of other approximate factors.
However, these negative variances can lead to failure of the proposed EP algorithm
 - details in the Supplementary Material.
To address this problem, whenever an EP update yields a negative number for any of the
$\hat{v}_{u,i}^{g,a}$, $\hat{v}_{u,d}^{w,a}$ or $\hat{v}_{d,j}^{h,a}$, we do not update this parameter, nor the corresponding
$\hat{m}_{u,i}^{g,a}$, $\hat{m}_{u,d}^{w,a}$ or $\hat{m}_{d,j}^{h,a}$.

\subsection{Sparse Approximations to speed up Computations} \label{sec:sparse}

The computational cost of EP is determined by the operations needed to refine the approximate factors $\hat{f}_3$ and $\hat{f}_4$.
Refinement of $\hat{f}_4$ required inversion of $D$ $P\times P$ matrices, which has cost $\mathcal{O}(DP^3)$. Similarly, for $\hat{f}_3$ we must invert $D$ $U\times U$ matrices, which has cost $\mathcal{O}(DU^3)$.
These costs can be prohibitive when $P$ or $U$ are very large.
Nevertheless, they can be reduced by using sparse approximations to the covariance matrices $\mathbf{K}_\text{users}$
and $\mathbf{K}_\text{items}$. We use the fully independent training conditional or FITC
approximation, also known as the sparse pseudo-input GP (SPGP) \cite{snelson2006}.
With FITC, the $U \times U$ covariance matrix $\mathbf{K}_\text{users}$ is approximated by
$\mathbf{K}_\text{users}' = \mathbf{Q}_\text{users} + \text{diag}(\mathbf{K}_\text{users}-\mathbf{Q}_\text{users})$,
where $\mathbf{Q}_\text{users} = \mathbf{K}_{\text{users},U,U_0} \mathbf{K}_{\text{users},U_0,U_0}^{-1} \mathbf{K}_{\text{users},U,U_0}^\text{T}$.
In this expression, $\mathbf{K}_{\text{users},U_0,U_0}$ is an $U_0 \times U_0$ covariance matrix given by the evaluation
of the covariance function for the users at all possible pairs of $U_0 < U$ locations
or \emph{user pseudo-inputs} $\{\mathbf{u}'_1,\ldots,\mathbf{u}'_{U_0}\}$,
where $\mathbf{u}'_i \in \mathcal{U}$ for $i = 1,\ldots,U_0$, and
$\mathbf{K}_{\text{users},U,U_0}$ is an $U \times U_0$ matrix with the evaluation of
the covariance function for the users at all possible pairs of original user feature vectors and user pseudo-inputs,
that is, $(\mathbf{u}_i, \mathbf{u}_j')$, for $i = 1,\ldots,U$ and $j = 1,\ldots,U_0$.
Similarly, the $P \times P$ covariance matrix $\mathbf{K}_\text{items}$ is also approximated by
$\mathbf{K}_\text{items}' = \mathbf{Q}_\text{items} + \text{diag}(\mathbf{K}_\text{items}-\mathbf{Q}_\text{items})$,
where $\mathbf{Q}_\text{items} = \mathbf{K}_{\text{items},P,P_0} \mathbf{K}_{\text{items},P_0,P_0}^{-1} \mathbf{K}_{\text{items},P,P_0}^\text{T}$,
$\mathbf{K}_{\text{items},P_0,P_0}$ is a $P_0 \times P_0$ covariance matrix given by the evaluation
of the preference kernel at all possible pairs of $P_0 < P$ locations or
\emph{item-pair pseudo-inputs} $\{(\mathbf{x}'_1,\mathbf{x}''_1),\ldots,(\mathbf{x}'_{P_0},\mathbf{x}''_{P_0})\}$,
where $\mathbf{x}'_i,\mathbf{x}''_i \in \mathcal{X}$ for $i = 1,\ldots,P_0$,
and $\mathbf{K}_{\text{items},P,P_0}$ is a $P \times P_0$ matrix with the evaluation of
the preference kernel at all possible combinations of feature vectors for the original item pairs and item-pair pseudo-inputs,
that is, $((\mathbf{x}_{\alpha(i)},\mathbf{x}_{\beta(i)}), (\mathbf{x}'_j,\mathbf{x}''_j))$, for $i = 1,\ldots,P$ and $j = 1,\ldots,P_0$.

Detailed description of how to refine the third and fourth approximate factors when $\mathbf{K}_\text{users}$ and
$\mathbf{K}_\text{items}$ are replaced by $\mathbf{K}_\text{users}'$ and $\mathbf{K}_\text{items}'$, respectively may be found in the Supplementary material.
The required operations are can be efficiently implemented using the formulas
described in \citep{Guzman2007} and \citep{Lazaro2010}.
The resulting complextiy of inference, after applying the FITC approximation to update $\hat{f}_3$ and $\hat{f}_4$ is $\mathcal{O}(DU_0^2U)$ and $\mathcal{O}(DP_0^2P)$ operations, respectively.

We now have a flexible multi-user model in which we can perform inference in reasonable computational time. In the following section we will compare our model to two current approached to proababilistic multi-user preference learning, and compare BALD to other popular algorithms for active learning.
