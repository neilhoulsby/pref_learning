\section{A Multi-User Model for Preference Learning \label{sec:model}}

Consider $I$ items with feature vectors $\mathbf{x}_i\in\mathcal{X}$ for $i=1,\ldots,I$. 
The single-user learning approach assumes an independent latent function for the $u$-th user,
$g_u:\mathcal{X}^2\mapsto\mathbb{R}$. Our approach to the multi-user problem is to assume common structure
in the user latent functions. In particular, we assume a set of $D$ shared latent functions,
$h_d:\mathcal{X}^2\mapsto \mathbb{R}$ for $d=1,\ldots,D$, such that the user latent functions are 
generated by a linear combination of these functions, namely

\begin{equation}
g_{u}(\mathbf{x}_j,\mathbf{x}_k)=\sum_{d=1}^{D}w_{u,d}h_{d}(\mathbf{x}_j,\mathbf{x}_k)\,,\label{eq:expressionG}
\end{equation}

here $w_{u,d}\in \mathbb{R}$ is the weight given to function $h_d$ for user $u$.
We place a GP prior over the shared latent functions $h_{1},\ldots,h_{D}$ using the
preference kernel described in the previous section.
This model allows the preferences of the different users to share
some common structure represented by the latent functions $h_{1},\ldots,h_{D}$.
This approach is similar to unsupervised dimensionality reduction methods that are commonly used for addressing collaborative filtering problems \cite{stern2009,raiko2007}.

We may extend this model further to the case in which, for each user $u$, there is
a feature vector $\mathbf{u}_u \in \mathcal{U}$ containing information that might be useful for prediction.
We denote by $\mathbf{U}$ the set of all the users' feature vectors,
that is, $\mathbf{U} = \{\mathbf{u}_1,\ldots,\mathbf{u}_U\}$.
The user features are incorporated now by placing a separate GP prior over the users weights.
In particular, we replace the scalars $w_{u,d}$ in (\ref{eq:expressionG}) with functions
$w_d'(\mathbf{u}_u):\mathcal{U}\rightarrow\mathcal{\mathbb{R}}$. 
These weight functions describe the contribution of shared latent function
$h_d$ to the user latent function $g_u$ as a function of the user feature vector $\mathbf{u}_u$.

In the multi-user setting we are given a list
$\List=\{p_1,\ldots,p_P\}$ with all the \emph{pairs} of items evaluated by the users, where $P\leq I(I-1)/2$ (the maximum number of pairs).
The data consists of $\List$, the sets of feature vectors for the users $\mathbf{U}$ (if available)
and the items $\mathbf{X}=\{\mathbf{x}_1,\ldots,\mathbf{x}_I\}$, and $U$ different sets of preference judgements,
one for each user, $\mathcal{D}=\{\{z_{u,i},y_{u,i}\}_{i=1}^{M_u}\}_{u=1}^{U}$, where $z_{u,i}$ indexes the $i$-th
pair evaluated by user $u$, $y_{i,u}=1$ if this user
prefers the first item in the pair to the second and $y_{i,u}=-1$ otherwise. $M_u$ is the number of 
preference judgements made by the $u$-th user.

\subsection{Probabilistic Description}

The task of interest is to predict preference on unseen item pairs for a particular user.
To do this we cast the model described above into a probabilistic framework.
Let $\mathbf{G}$ be an $U\times P$ `user-function' matrix, where each row corresponds to
a particular user's latent function, that is, the entry in the $u$-th column and $i$-th row is 
$g_{u,i}= g_u(\mathbf{x}_{\alpha(i)},\mathbf{x}_{\beta(i)})$
and $\alpha(i)$ and $\beta(i)$ denote respectively the first and second item in the $i$-th pair from $\mathcal{L}$.
Let $\mathbf{H}$ be a $D\times P$ `shared-function' matrix,
where each row represents the shared latent functions, that is, the entry in the $d$-th row and $i$-th column is 
$h_{d,i}= h_d(\mathbf{x}_{\alpha(i)},\mathbf{x}_{\beta(i)})$.
Finally, we introduce the $U \times D$ weight matrix $\mathbf{W}$ such that each row contains a user's weights, that is, the entry in the $u$-th row and $d$-th column of this matrix is $w_d'(\mathbf{u}_u)$.
Note that $\mathbf{G} = \mathbf{W} \mathbf{H}$ represents equation (\ref{eq:expressionG}) in matrix form.
Let $\mathbf{T}$ be the $U\times P$ target matrix given by $\mathbf{T} = \text{sign}[\mathbf{G} + \mathbf{E}]$,
where $\mathbf{E}$ is an $U \times P$ noise matrix whose entries are sampled i.i.d. from a standard Gaussian distribution and
the function ``$\text{sign}$'' retains only the sign of the elements in a matrix. 
The observations $y_{u,i}$ in $\mathcal{D}=\{\{z_{u,i},y_{u,i}\}_{i=1}^{M_u}\}_{u=1}^{U}$ are
mapped to the corresponding entries of $\mathbf{T}$ using $t_{u,z_{u,i}} = y_{u,i}$.
Let $\mathbf{T}^{(\mathcal{D})}$ and $\mathbf{G}^{(\mathcal{D})}$ represent the elements of $\mathbf{T}$ and $\mathbf{G}$
corresponding only to the available observations $y_{u,i}$ in $\mathcal{D}$.
Then, the likelihood for $\mathbf{G}^{(\mathcal{D})}$ given $\mathbf{T}^{(\mathcal{D})}$ and conditional distribution for $\mathbf{G}^{(\mathcal{D})}$ given $\mathbf{H}$ and $\mathbf{W}$ are

\begin{align*}
\mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{G}^{(\mathcal{D})}) 
= \prod_{u=1}^U \prod_{i=1}^{M_u} \Phi[t_{u,z_{u,i}} g_{u,z_{u,i}}]\,\,\,\text{and}\,\,\,
\mathcal{P}(\mathbf{G}^{(\mathcal{D})}|\mathbf{W},\mathbf{H}) = 
\prod_{u=1}^{U} \prod_{i=1}^{M_u}\delta[g_{u,z_{u,i}}-\mathbf{w}_u\mathbf{h}_{\cdot,z_{u,i}}]\,
\end{align*}

respectively, where $\mathbf{w}_u$ is the $u$-th row in $\mathbf{W}$, $\mathbf{h}_{\cdot,i}$ is the $i$-th column in $\mathbf{H}$
and $\delta$ represents a point probability mass at zero.
We now select the priors for $\mathbf{W}$ and $\mathbf{H}$. 
We assume that each function $w_1',\ldots,w_D'$ is sampled \textit{a priori} from a GP
with zero mean and specific covariance function. Let $\mathbf{K}_\text{users}$ be the $U \times U$ 
covariance matrix for entries in each column of matrix $\mathbf{W}$. Then

\begin{equation}
\mathcal{P}(\mathbf{W}|\mathbf{U})=  
\prod_{d=1}^D \mathcal{N}(\mathbf{w}_{\cdot,d}|\mathbf{0},\mathbf{K}_\text{users})\,,\label{eq:priorW}
\end{equation}

where $\mathbf{w}_{\cdot,d}$ is the $d$-th column in $\mathbf{W}$.
If user features are unavailable, $\mathbf{K}_\text{users}$ becomes the identity matrix.
Finally, we assume that each shared latent function $h_1,\ldots,h_D$ is sampled \textit{a priori} from a GP
with zero mean and covariance function given by a preference kernel. 
Let $\mathbf{K}_\text{items}$ be the $P \times P$ preference covariance 
matrix for the item pairs in $\List$. The prior for $\mathbf{H}$ is then 

\begin{equation}
\mathcal{P}(\mathbf{H}|\mathbf{X},\List) = 
\prod_{j=1}^{D}\mathcal{N}(\mathbf{h}_j|\mathbf{0},\mathbf{K}_\text{items})\,,\label{eq:priorH}
\end{equation}

where $\mathbf{h}_j$ is the $j$-th row in $\mathbf{H}$. The resulting posterior for $\mathbf{W}$, $\mathbf{H}$ and $\mathbf{G}^{(\mathcal{D})}$ is

\begin{equation}
\mathcal{P}(\mathbf{W},\mathbf{H},\mathbf{G}^{(\mathcal{D})}|\mathbf{T}^{(\mathcal{D})},\mathbf{X},\List) =
\frac{\mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{G}^{(\mathcal{D})})
\mathcal{P}(\mathbf{G}^{(\mathcal{D})}|\mathbf{W},\mathbf{H})\mathcal{P}(\mathbf{W}|\mathbf{U})\mathcal{P}
(\mathbf{H}|\mathbf{X},\List)}{\mathcal{P}(\mathbf{T}^{(\mathcal{D}}|\mathbf{X},\List)}\,.\label{eq:post}
\end{equation}

Given a new item pair $p_{P+1}$, we can compute the predictive distribution for the preference of the $u$-th user ($1 \leq u \leq U$) on this pair by integrating out the parameters $\mathbf{H},\mathbf{W}$ and $\mathbf{G}^{(\mathcal{D})}$ as follows:

\begin{align}
\mathcal{P}(t_{u,P+1}|&\mathbf{T}^{(\mathcal{D})},\mathbf{X},\List,p_{P+1}) =
\int \mathcal{P}(t_{u,P+1}|g_{u,P+1}) \mathcal{P}(g_{u,P+1}|\mathbf{w}_u,\mathbf{h}_{\cdot,P+1})\notag\\
 & \quad \mathcal{P}(\mathbf{h}_{\cdot,P+1}|\mathbf{H},\mathbf{X},\List,p_{P+1})
\mathcal{P}(\mathbf{H},\mathbf{W},\mathbf{G}^{(\mathcal{D})}|\mathbf{T}^{(\mathcal{D})},\mathbf{X},\List)
\,d\mathbf{H}\,d\mathbf{W}\,d\mathbf{G}^{(\mathcal{D})}\,,
\label{eq:predictions}
\end{align}
where $\mathcal{P}(t_{u,P+1}|g_{u,P+1})=\Phi[t_{u,P+1}g_{u,P+1}]$,
$\mathcal{P}(g_{u,P+1}|\mathbf{w}_u,\mathbf{h}_{\cdot,P+1})=\delta[ g_{u,P+1} - \mathbf{w}_u \mathbf{h}_{\cdot,P+1}]$,
\begin{equation}
\mathcal{P}(\mathbf{h}_{\cdot,P+1}|\mathbf{H},\mathbf{X},\List,p_{P+1})
=\prod_{d=1}^D \mathcal{N}(h_{d,P+1}|\mathbf{k}_\star^\text{T} \mathbf{K}^{-1}_\text{items} \mathbf{h}_d, k_\star -
\mathbf{k}_\star^\text{T}  \mathbf{K}^{-1}_\text{items} \mathbf{k}_\star)
\label{eq:predictive}
\end{equation}
$k_\star$ is the prior variance of $h_d(\mathbf{x}_{\alpha(P+1)}, \mathbf{x}_{\beta(P+1)})$
and $\mathbf{k}_\star$ is a $P$-dimensional vector that contains the prior covariances between $h_d(\mathbf{x}_{\alpha(P+1)}, \mathbf{x}_{\beta(P+1)})$
and $h_d(\mathbf{x}_{\alpha(1)}, \mathbf{x}_{\beta(1)}),\ldots,h_d(\mathbf{x}_{\alpha(P)}, \mathbf{x}_{\beta(P)})$.

In practice, computing (\ref{eq:post}) or (\ref{eq:predictive})
is infeasible and approximate inference has to be used.
For this task, we propose to use a combination of expectation propagation (EP) \cite{Minka2001} and variation Bayes (VB) \cite{Attias1999}.
Empirical studies show that EP obtains state-of-the-art performance 
in the related problem of GP binary classification \cite{nickisch2008}.

Typically, data with which to learn the model is limited, and furthermore the computational 
time required to train and make predictions can become large when the volume of data becomes very large (there is further discussion of computational complexity in Sections \ref{sec:sparse} and \ref{sec:relatedWork}).  
Therefore, we want to learn about the users' preferences with the proposed model
using the least amount of data possible, or equivalently, extract maximal useful information from a 
limited number os observations. Therefore we query
users actively about their preferences on the most informative pairs of items \cite{brochu2007}.
Next, we describe a novel method to implement this strategy.
This method exploits the preference kernel and so may
be trivially generalized to GP binary classification problems also.
