\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{birlutiu2009}
\citation{brochu2007}
\citation{de2009}
\citation{furnkranz2010}
\citation{chu2005}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{birlutiu2009,Bonilla2010}
\citation{chu2005}
\citation{koren2008,stern2009}
\citation{Minka2001,Attias1999}
\citation{chu2005}
\citation{birlutiu2009,Bonilla2010}
\citation{chu2005,Bonilla2010}
\citation{chu2005}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Summary of key notation.}}{3}{figure.1}}
\newlabel{fig:notation}{{1}{3}{Summary of key notation}{figure.1}{}}
\newlabel{sec:prefKernel}{{2}{3}{Pairwise Preference Learning as Special Case of Binary Classification \label {sec:prefKernel}\relax }{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Pairwise Preference Learning as Special Case of Binary Classification }{3}{section.2}}
\citation{furnkranz2010}
\citation{chu2005}
\citation{chu2005}
\newlabel{eq:likelihood}{{1}{4}{Pairwise Preference Learning as Special Case of Binary Classification \label {sec:prefKernel}\relax }{equation.2.1}{}}
\newlabel{eq:likelihood2}{{3}{4}{Pairwise Preference Learning as Special Case of Binary Classification \label {sec:prefKernel}\relax }{equation.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Generative model underlying the preference learning framework. \emph  {Left:} the original approach \citep  {chu2005} considers the latent preference function $f$ as latent parameter, and the rest of the graphical model as a complex, structured likelihood. \emph  {Right:} Our approach re-parametrises the problem in terms of $g$, and thus works with a simpler likelihood but with a more structured prior. The prior takes the form of a Gaussian Process with the preference judgement covariance function. }}{5}{figure.2}}
\newlabel{fig:graphical_model}{{2}{5}{Generative model underlying the preference learning framework. \emph {Left:} the original approach \citep {chu2005} considers the latent preference function $f$ as latent parameter, and the rest of the graphical model as a complex, structured likelihood. \emph {Right:} Our approach re-parametrises the problem in terms of $g$, and thus works with a simpler likelihood but with a more structured prior. The prior takes the form of a Gaussian Process with the preference judgement covariance function. \label {fig:graphical_model}\relax }{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Properties of the Preference Kernel}{5}{subsection.2.1}}
\citation{stern2009,raiko2007}
\newlabel{sec:model}{{3}{6}{A Multi-User Model for Preference Learning \label {sec:model}\relax }{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}A Multi-User Model for Preference Learning }{6}{section.3}}
\newlabel{eq:expressionG}{{5}{6}{A Multi-User Model for Preference Learning \label {sec:model}\relax }{equation.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Probabilistic Description}{6}{subsection.3.1}}
\citation{Minka2001}
\citation{Attias1999}
\citation{nickisch2008}
\citation{brochu2007}
\newlabel{eq:priorW}{{6}{7}{Probabilistic Description\relax }{equation.3.6}{}}
\newlabel{eq:priorH}{{7}{7}{Probabilistic Description\relax }{equation.3.7}{}}
\newlabel{eq:post}{{8}{7}{Probabilistic Description\relax }{equation.3.8}{}}
\newlabel{eq:predictions}{{9}{7}{Probabilistic Description\relax }{equation.3.9}{}}
\newlabel{eq:predictive}{{10}{7}{Probabilistic Description\relax }{equation.3.10}{}}
\citation{lindley1956,bernardo1979}
\citation{lawrence2002}
\citation{freund1997}
\citation{tong2001}
\citation{coverandthomas}
\citation{heckerman1995}
\citation{dasgupta2005,golovin2010}
\newlabel{sec:active}{{4}{8}{Active Learning \label {sec:active}\relax }{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Active Learning }{8}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Information Theoretic Approach}{8}{subsection.4.1}}
\citation{mackay1992,krishnapuram2004,lawrence2002}
\citation{panzeri2007}
\citation{mackay1992,krishnapuram2004,lawrence2002}
\newlabel{eqn:ent_change}{{11}{9}{Information Theoretic Approach\relax }{equation.4.11}{}}
\newlabel{eqn:rearrangement}{{12}{9}{Information Theoretic Approach\relax }{equation.4.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Application to GP Preference Learning}{9}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Analytic approximation ({\relax \fontsize  {7}{8}\selectfont  $\mathrel {\mathop {\approx }\limits ^{1}}$}) to the binary entropy of the error function by a squared exponential. The absolute error remains under $3 \cdot 10^{-3}$. }}{10}{figure.3}}
\newlabel{fig:trick}{{3}{10}{Analytic approximation ({\scriptsize $\stackrel {1}{\approx }$}) to the binary entropy of the error function by a squared exponential. The absolute error remains under $3 \cdot 10^{-3}$. \label {fig:trick}\relax }{figure.3}{}}
\newlabel{ent_mean}{{13}{10}{Application to GP Preference Learning\relax }{equation.4.13}{}}
\newlabel{eqn:mean_entropy}{{14}{10}{Application to GP Preference Learning\relax }{equation.4.14}{}}
\newlabel{eqn:BALD}{{15}{10}{Application to GP Preference Learning\relax }{equation.4.15}{}}
\citation{Minka2002,gerven2010a}
\citation{stern2009}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Toy classification example with a 1D input. Circles and crosses are the data. We plot the mean and variance of the predictive distribution. Maximum Entropy Sampling (MES, see Section \ref  {sec:relatedWork}) samples from the region of highest marginal uncertainty, ignoring the second term in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eqn:rearrangement}\unskip \@@italiccorr )}}. BALD samples from the region of greatest model uncertainty i.e. the area with the largest shaded error-area. }}{11}{figure.4}}
\newlabel{fig:BALD}{{4}{11}{Toy classification example with a 1D input. Circles and crosses are the data. We plot the mean and variance of the predictive distribution. Maximum Entropy Sampling (MES, see Section \ref {sec:relatedWork}) samples from the region of highest marginal uncertainty, ignoring the second term in \eqref {eqn:rearrangement}. BALD samples from the region of greatest model uncertainty i.e. the area with the largest shaded error-area. \label {fig:BALD}\relax }{figure.4}{}}
\newlabel{sec:ep}{{5}{11}{Expectation Propagation and Variational Bayes\label {sec:ep}\relax }{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Expectation Propagation and Variational Bayes}{11}{section.5}}
\newlabel{eq:epPostApprox}{{16}{11}{Expectation Propagation and Variational Bayes\label {sec:ep}\relax }{equation.5.16}{}}
\citation{stern2009}
\citation{Bishop2007}
\citation{Minka2001}
\citation{Minka2002}
\newlabel{eq:KL}{{20}{12}{Expectation Propagation and Variational Bayes\label {sec:ep}\relax }{equation.5.20}{}}
\citation{HernandezLobato2010}
\newlabel{eq:damping}{{21}{13}{Expectation Propagation and Variational Bayes\label {sec:ep}\relax }{equation.5.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}The EP predictive distribution}{13}{subsection.5.1}}
\newlabel{eq:predictiveMean}{{25}{13}{The EP predictive distribution\relax }{equation.5.25}{}}
\newlabel{eq:predictiveVariance}{{26}{13}{The EP predictive distribution\relax }{equation.5.26}{}}
\citation{Minka2001,Minka2002}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}The EP update operations}{14}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}The EP Approximation of the Model Evidence}{14}{subsection.5.3}}
\citation{snelson2006}
\citation{Guzman2007}
\citation{Lazaro2010}
\citation{birlutiu2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Sparse Approximations to speed up Computations}{15}{subsection.5.4}}
\newlabel{sec:sparse}{{5.4}{15}{Sparse Approximations to speed up Computations\relax }{subsection.5.4}{}}
\newlabel{sec:relatedWork}{{6}{15}{Related Methods \label {sec:relatedWork}\relax }{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Methods }{15}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Multi-User Preference Learning}{15}{subsection.6.1}}
\@writefile{toc}{\contentsline {paragraph}{Model of Birlutiu et al.}{15}{section*.1}}
\citation{birlutiu2009}
\citation{nickisch2008}
\citation{Bonilla2010}
\citation{Bonilla2010}
\citation{nickisch2008}
\@writefile{toc}{\contentsline {paragraph}{Model of Bonilla et al.}{16}{section*.2}}
\newlabel{eq:covBonilla}{{35}{16}{Model of Bonilla et al}{equation.6.35}{}}
\citation{sebastiani2000}
\citation{freund1997}
\citation{lawrence2002}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Active Learning}{17}{subsection.6.2}}
\@writefile{toc}{\contentsline {paragraph}{Maximum Entropy Sampling}{17}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{Query by Committee}{17}{section*.4}}
\@writefile{toc}{\contentsline {paragraph}{The Informative Vector Machine}{17}{section*.5}}
\citation{tong2001}
\citation{tong2001}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance of BALD and other active learning algorithms on several binary classification datasets from the UCI repository. Entries indicate the number of datapoints (plus or minus one standard error of the mean) required to achieve $95\%$ of the predictive performance achieved by including the entire pool set. Bold typeface indicates the best performing algorithm for each dataset. N/A indicates that the corresponding algorithms did not meet the $95\%$ performance level by the end of the simulation. }}{18}{table.1}}
\newlabel{tab:activeTable}{{1}{18}{Performance of BALD and other active learning algorithms on several binary classification datasets from the UCI repository. Entries indicate the number of datapoints (plus or minus one standard error of the mean) required to achieve $95\%$ of the predictive performance achieved by including the entire pool set. Bold typeface indicates the best performing algorithm for each dataset. N/A indicates that the corresponding algorithms did not meet the $95\%$ performance level by the end of the simulation.\label {tab:activeTable} \relax }{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Binary Classification Investigation}{18}{subsection.6.3}}
\citation{Atteia1994315,Webster1994}
\citation{Kamishima05}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experiments}{19}{section.7}}
\newlabel{sec:experiments}{{7}{19}{Experiments\relax }{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Datasets}{19}{subsection.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Tuning the kernel lengthscale}{19}{subsection.7.2}}
\citation{MacKay2001}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Logarithm of the evidence returned by EP when run on the first training set of the experiments with synthetic data. Different values are considered for the lengthscale parameters $\sigma _\text  {users}$ and $\sigma _\text  {items}$. The synthetic data are generated using $\qopname  \relax o{log}\sigma _\text  {users} = 0$ and $\qopname  \relax o{log}\sigma _\text  {items} = 0$. The highest evidence returned by EP corresponds to values of $\qopname  \relax o{log}\sigma _\text  {users}$ and $\qopname  \relax o{log}\sigma _\text  {items}$ close to zero.}}{20}{figure.5}}
\newlabel{fig:experimentEvidence}{{5}{20}{Logarithm of the evidence returned by EP when run on the first training set of the experiments with synthetic data. Different values are considered for the lengthscale parameters $\sigma _\text {users}$ and $\sigma _\text {items}$. The synthetic data are generated using $\log \sigma _\text {users} = 0$ and $\log \sigma _\text {items} = 0$. The highest evidence returned by EP corresponds to values of $\log \sigma _\text {users}$ and $\log \sigma _\text {items}$ close to zero}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Comparison with other multi-user methods}{20}{subsection.7.3}}
\@writefile{toc}{\contentsline {paragraph}{Alternative models.}{20}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{Experimental procedure.}{20}{section*.7}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average test error with 100 users.}}{21}{table.2}}
\newlabel{tab:errorSmallDatasets}{{2}{21}{Average test error with 100 users}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Training times (s) with 100 users.}}{21}{table.3}}
\newlabel{tab:timeSmallDatasets}{{3}{21}{Training times (s) with 100 users}{table.3}{}}
\newlabel{tab:small}{{7.3}{21}{Experimental procedure}{table.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Results.}{21}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Active learning on large datasets}{21}{subsection.7.4}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Test error for each method and active learning strategy with at most 1000 users. Standard deviations ommitted for readability.}}{22}{table.4}}
\newlabel{tab:large}{{4}{22}{Test error for each method and active learning strategy with at most 1000 users. Standard deviations ommitted for readability}{table.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Average test error for CPU, CP and SU, using the strategies BALD (-B), entropy (-E) and random (-R) for active learning. For visual clarity, the curves for CPU are included only in the Synthetic and Election datasets.}}{22}{figure.6}}
\newlabel{fig:learningcurves}{{6}{22}{Average test error for CPU, CP and SU, using the strategies BALD (-B), entropy (-E) and random (-R) for active learning. For visual clarity, the curves for CPU are included only in the Synthetic and Election datasets}{figure.6}{}}
\citation{joachims2005}
\citation{girard2003}
\bibstyle{apalike}
\bibdata{bib/bibliog}
\bibcite{Atteia1994315}{{1}{1994}{{Atteia et~al.}}{{}}}
\bibcite{Attias1999}{{2}{1999}{{Attias}}{{}}}
\bibcite{bernardo1979}{{3}{1979}{{Bernardo}}{{}}}
\newlabel{sec:conclusions}{{8}{23}{Conclusions \label {sec:conclusions}\relax }{section.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusions }{23}{section.8}}
\bibcite{birlutiu2009}{{4}{2010}{{Birlutiu et~al.}}{{}}}
\bibcite{Bishop2007}{{5}{2007}{{Bishop}}{{}}}
\bibcite{Bonilla2010}{{6}{2010}{{Bonilla et~al.}}{{}}}
\bibcite{brochu2007}{{7}{2007}{{Brochu et~al.}}{{}}}
\bibcite{chu2005}{{8}{2005}{{Chu and Ghahramani}}{{}}}
\bibcite{coverandthomas}{{9}{1991}{{Cover et~al.}}{{}}}
\bibcite{dasgupta2005}{{10}{2005}{{Dasgupta}}{{}}}
\bibcite{de2009}{{11}{2009}{{De~Gemmis et~al.}}{{}}}
\bibcite{freund1997}{{12}{1997}{{Freund et~al.}}{{}}}
\bibcite{furnkranz2010}{{13}{2010}{{F{\"u}rnkranz and H{\"u}llermeier}}{{}}}
\bibcite{girard2003}{{14}{2003}{{Girard et~al.}}{{}}}
\bibcite{golovin2010}{{15}{2010}{{Golovin and Krause}}{{}}}
\bibcite{Lazaro2010}{{16}{2010}{{Gredilla}}{{}}}
\bibcite{heckerman1995}{{17}{1995}{{Heckerman et~al.}}{{}}}
\bibcite{HernandezLobato2010}{{18}{2010}{{Hern\'andez-Lobato}}{{}}}
\bibcite{joachims2005}{{19}{2005}{{Joachims et~al.}}{{}}}
\bibcite{Kamishima05}{{20}{2005}{{Kamishima et~al.}}{{}}}
\bibcite{koren2008}{{21}{2008}{{Koren}}{{}}}
\bibcite{krishnapuram2004}{{22}{2004}{{Krishnapuram et~al.}}{{}}}
\bibcite{lawrence2002}{{23}{2002}{{Lawrence et~al.}}{{}}}
\bibcite{lindley1956}{{24}{1956}{{Lindley}}{{}}}
\bibcite{mackay1992}{{25}{1992}{{MacKay}}{{}}}
\bibcite{MacKay2001}{{26}{2001}{{MacKay}}{{}}}
\bibcite{Minka2001}{{27}{2001}{{Minka}}{{}}}
\bibcite{Minka2002}{{28}{2002}{{Minka and Lafferty}}{{}}}
\bibcite{Guzman2007}{{29}{2007}{{Naish-Guzman and Holden}}{{}}}
\bibcite{nickisch2008}{{30}{2008}{{Nickisch and Rasmussen}}{{}}}
\bibcite{panzeri2007}{{31}{2007}{{Panzeri~S. and R.S.}}{{}}}
\bibcite{raiko2007}{{32}{2007}{{Raiko et~al.}}{{}}}
\bibcite{sebastiani2000}{{33}{2000}{{Sebastiani and Wynn}}{{}}}
\bibcite{snelson2006}{{34}{2005}{{Snelson and Ghahramani}}{{}}}
\bibcite{stern2009}{{35}{2009}{{Stern et~al.}}{{}}}
\bibcite{tong2001}{{36}{2001}{{Tong and Koller}}{{}}}
\bibcite{gerven2010a}{{37}{2010}{{van Gerven et~al.}}{{}}}
\bibcite{Webster1994}{{38}{1994}{{Webstet et~al.}}{{}}}
